{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     89,
     116,
     125,
     164,
     192,
     350,
     413,
     563,
     629,
     769,
     782,
     824,
     874,
     884,
     914,
     946,
     1220,
     1232,
     1245,
     1253,
     1260,
     1267,
     1277,
     1284,
     1347
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions\n",
      "Tensorflow :  2.1.0\n",
      "Pandas     :  0.24.2\n",
      "Numpy      :  0.24.2\n",
      "Sklearn    :  0.22.1\n",
      "NO GPUs available\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n",
      "CMD: pwd\n",
      "OUT: /home/ec2-user/SageMaker\n",
      "----------\n",
      "CMD: ls\n",
      "OUT: banksim1.zip\n",
      "best_estimator_.history.txt\n",
      "best_estimator__rnn.history\n",
      "best_estimator_.txt\n",
      "best_index__rnn.history\n",
      "BEST MODEL ACC.png\n",
      "BEST MODEL LOSS.png\n",
      "best_params__rnn.history\n",
      "best_score__rnn.history\n",
      "bs140513_032310.csv\n",
      "bsNET140513_032310.csv\n",
      "customer_batches_rnn_best_model.h5\n",
      "cv_results.csv\n",
      "cv_results.history\n",
      "cv_results__rnn.history\n",
      "cv_rnn.history\n",
      "error_score_rnn.history\n",
      "estimator.history.txt\n",
      "estimator_rnn.history\n",
      "estimator.txt\n",
      "iid_rnn.history\n",
      "kernel-setup.err\n",
      "kernel-setup.out\n",
      "kernel-setup.sh\n",
      "labels_hash.data\n",
      "logs\n",
      "lost+found\n",
      "Makefile\n",
      "Model#1_acc.png\n",
      "Model#1_auc.png\n",
      "Model#1_prec.png\n",
      "Model#1_rec.png\n",
      "Model#2_acc.png\n",
      "Model#2_auc.png\n",
      "Model#2_prec.png\n",
      "Model#2_rec.png\n",
      "multimetric__rnn.history\n",
      "n_jobs_rnn.history\n",
      "n_splits__rnn.history\n",
      "param_grid_rnn.history\n",
      "PRCURVE.png\n",
      "pre_dispatch_rnn.history\n",
      "refit_rnn.history\n",
      "refit_time__rnn.history\n",
      "results\n",
      "return_train_score_rnn.history\n",
      "rnn_data.data\n",
      "RNN_Detection.ipynb\n",
      "rnn_fraud_cust_batches_v1.py\n",
      "rnn.history\n",
      "rnn_mod_data.data\n",
      "rnn_model.h5\n",
      "rnn_model_weights.h5\n",
      "ROCAUC.png\n",
      "scaler.data\n",
      "scorer__rnn.history\n",
      "scoring_rnn.history\n",
      "SlidingWindow-RNN_Fraud_Detection.py\n",
      "train_callback_log.txt\n",
      "verbose_rnn.history\n",
      "X_test.data\n",
      "X_train.data\n",
      "X_val.data\n",
      "y_test.data\n",
      "y_train.data\n",
      "y_val.data\n",
      "----------\n",
      "CMD: ./trin\n",
      "OUT: [Errno 2] No such file or directory: './trin': './trin'\n",
      "----------\n",
      "\n",
      "    DATA PREPROCESSING\n",
      "    DOWNLOAD FROM KAGGLE: False\n",
      "    GENERATE DATA:        False\n",
      "    READ FROM CLOUD:      False\n",
      "    SAVE TO CLOUD:        False\n",
      "\n",
      "\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _  IMPORT DATA FROM CSV _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "\n",
      "Deleting the columns 'zipcodeOri','zipMerchant' because all the fields are equal.\n",
      "\n",
      "\n",
      "Data Shape: (594643, 8) \n",
      "\n",
      "Preview: \n",
      "\n",
      "    step       customer  age gender       merchant             category  amount  fraud\n",
      "0  0     'C1093826151'  '4'  'M'    'M348934600'   'es_transportation'  4.55    0    \n",
      "1  0     'C352968107'   '2'  'M'    'M348934600'   'es_transportation'  39.68   0    \n",
      "2  0     'C2054744914'  '4'  'F'    'M1823072687'  'es_transportation'  26.89   0    \n",
      "3  0     'C1760612790'  '3'  'M'    'M348934600'   'es_transportation'  17.25   0    \n",
      "4  0     'C757503768'   '5'  'M'    'M348934600'   'es_transportation'  35.72   0     \n",
      "\n",
      " Data Information: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 594643 entries, 0 to 594642\n",
      "Data columns (total 8 columns):\n",
      "step        594643 non-null int64\n",
      "customer    594643 non-null object\n",
      "age         594643 non-null object\n",
      "gender      594643 non-null object\n",
      "merchant    594643 non-null object\n",
      "category    594643 non-null object\n",
      "amount      594643 non-null float64\n",
      "fraud       594643 non-null int64\n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 36.3+ MB\n",
      "\n",
      "None\n",
      "Does it has null values? False\n",
      "\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _   READ DATA LOCALLY  _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SHAPES & KEYS:\n",
      "    X_train          : (285428, 25, 12)\n",
      "    y_train          : (285428,)\n",
      "    X_test           : (237858, 25, 12)\n",
      "    y_test           : (237858,)\n",
      "    X_val            : (71357, 25, 12)\n",
      "    y_val            : (71357,)\n",
      "    labels_hash Keys : dict_keys(['customer', 'age', 'gender', 'merchant', 'category'])\n",
      "    \n",
      "\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _  INITIALIZING GRID SEARCH RNN MODEL _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "\n",
      "\n",
      "        PARAMETERS:\n",
      "        ________________________________\n",
      "        input_shape :  (25, 12)\n",
      "        output_dim  :  1\n",
      "        main scoring:  recall\n",
      "        all scoring :  ['accuracy', 'precision', 'recall', 'roc_auc', 'f1', 'average_precision']\n",
      "        early_stopping_monitor   : val_recall\n",
      "        model_checkpoint_monitor : val_recall\n",
      "        verbose: 2\n",
      "        \n",
      "rnn_hidden_layers : [0, 1]\n",
      "rnn_hidden_layers_neurons : [50, 100]\n",
      "hidden_layers : [2]\n",
      "hidden_layers_neurons : [200, 300]\n",
      "loss : ['binary_crossentropy']\n",
      "optimizer : ['adam']\n",
      "modelType : ['LSTM', 'GRU']\n",
      "epochs : [50]\n",
      "output_layer_activation : ['sigmoid']\n",
      "rnn_layer_activation : ['sigmoid']\n",
      "hidden_layer_activation : ['sigmoid']\n",
      "dropout : [True]\n",
      "dropout_rate : [0.2]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _  TRAINING RNN _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "\n",
      "\n",
      "        Class weights: \n",
      "[ 0.50613724 41.23490321]\n",
      "{0: 0.506137243010707, 1: 41.23490320716556}\n",
      "\n",
      "        for classes: \n",
      "[0. 1.]\n",
      "\n",
      "        # Frauds: 3461\n",
      "        # of Non-Frauds: 281967\n",
      "        \n",
      "INPUTS\n",
      "        X:      (285428, 25, 12)\n",
      "        y:      (285428,)\n",
      "        X_test: (237858, 25, 12)\n",
      "        y_test: (237858,)\n",
      "        \n",
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n",
      "[CV] dropout=True, dropout_rate=0.2, epochs=50, hidden_layer_activation=sigmoid, hidden_layers=2, hidden_layers_neurons=200, loss=binary_crossentropy, modelType=LSTM, optimizer=adam, output_layer_activation=sigmoid, rnn_hidden_layers=0, rnn_hidden_layers_neurons=50, rnn_layer_activation=sigmoid \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _  CREATING ML MODEL _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "\n",
      "\n",
      "        PARAMETERS:\n",
      "        ________________________________ \n",
      "          rnn_hidden_layers:         0 \n",
      "          rnn_hidden_layers_neurons: 50 \n",
      "          hidden_layers:             2 \n",
      "          hidden_layers_neurons:     200\n",
      "          loss:                      binary_crossentropy\n",
      "          optimizer:                 adam\n",
      "          modelType:                 LSTM\n",
      "          dropout:                   True\n",
      "          dropout_rate:              0.2\n",
      "          input_shape:               (25, 12)\n",
      "          output_dim:                1\n",
      "          output_layer_activation:   sigmoid\n",
      "          rnn_layer_activation:      sigmoid\n",
      "          hidden_layer_activation:   sigmoid\n",
      "          keras_eval_metric:         [[<tensorflow.python.keras.metrics.TruePositives object at 0x7f12537378d0>, <tensorflow.python.keras.metrics.FalsePositives object at 0x7f12501d7438>, <tensorflow.python.keras.metrics.TrueNegatives object at 0x7f12501d76d8>, <tensorflow.python.keras.metrics.FalseNegatives object at 0x7f12501d74e0>, <tensorflow.python.keras.metrics.BinaryAccuracy object at 0x7f12523469b0>, <tensorflow.python.keras.metrics.Precision object at 0x7f125045b2b0>, <tensorflow.python.keras.metrics.Recall object at 0x7f125045ba20>, <tensorflow.python.keras.metrics.AUC object at 0x7f125045b8d0>]]\n",
      "          callbacks:                 [<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f1350c90e48>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f1350c90dd8>, <tensorflow.python.keras.callbacks.CSVLogger object at 0x7f146e2fd048>, <tensorflow.python.keras.callbacks.TensorBoard object at 0x7f13ac5d4ef0>, <tensorflow.python.keras.callbacks.ProgbarLogger object at 0x7f1349c59668>]\n",
      "          \n",
      "\n",
      "Model: \"sequential_112\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_162 (LSTM)              (None, 50)                12600     \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 1)                 201       \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 63,201\n",
      "Trainable params: 63,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "MODEL SUMMARY: \n",
      "\n",
      " None\n",
      "Train on 256885 samples, validate on 237858 samples\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_recall` which is not available. Available metrics are: loss,tp,fp,tn,fn,accuracy,precision,recall,auc\n",
      "WARNING:tensorflow:Can save best model only with val_recall available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-aec143f9343b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1428\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-aec143f9343b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, X_test, y_test)\u001b[0m\n\u001b[1;32m   1183\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m           \u001b[0mclass_weight\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m           \u001b[0mcallbacks\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m         )\n\u001b[1;32m   1187\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n{} {} {}\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\"_ \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"RNN TRAINING RESULTS\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\"_ \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/verafin-mitacs-2020/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[179]:\n",
    "\n",
    "\n",
    "download_data   = False    # Download from Kaggle\n",
    "generate_data   = False     # Preprocess Data. If it is false it reads the data locally.\n",
    "read_from_cloud = False    # Download Preprocessed Data From Cloud. If it is false it reads the files locally.\n",
    "save_to_cloud   = False    # Save to cloud. \n",
    "bucket_address  = \"s3://verafin-mitacs-ruben-chevez/\"\n",
    "project_folder  = \"customer_batches\"\n",
    "model_name      = \"customer_batches_rnn_best_model.h5\"\n",
    "project_path    = bucket_address + project_folder \n",
    "empty_padding_value           = -1\n",
    "reduce_data_for_testing       = False\n",
    "reduce_data_for_testing_value = 100\n",
    "\n",
    "\n",
    "# # Libraries & Requirements\n",
    "# \n",
    "\n",
    "# ## Libraries\n",
    "\n",
    "# In[149]:\n",
    "\n",
    "\n",
    "#General\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "import subprocess\n",
    "import math\n",
    "import time\n",
    "import progressbar\n",
    "import pickle\n",
    "import joblib\n",
    "import s3fs\n",
    "from pathlib import Path\n",
    "\n",
    "#Math & Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Oversampling\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "## Metrics\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold , StratifiedKFold\n",
    "\n",
    "## Models\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.backend.tensorflow_backend import set_session\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.metrics import Precision, Recall, PrecisionAtRecall\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping,  ModelCheckpoint, CSVLogger, TensorBoard, ProgbarLogger\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Versions\")\n",
    "print(\"Tensorflow : \", tf.__version__)\n",
    "print(\"Pandas     : \", pd.__version__)\n",
    "print(\"Numpy      : \", pd.__version__) \n",
    "print(\"Sklearn    : \", sklearn.__version__) \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        # tf.config.gpu.set_per_process_memory_fraction(0.4)\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"NO GPUs available\")\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "\n",
    "# ## Utils\n",
    "\n",
    "# In[150]:\n",
    "\n",
    "\n",
    "def runCommand(cmd):\n",
    "    try:\n",
    "        stream = os.popen(cmd)\n",
    "        output = stream.read().strip()\n",
    "    except Exception as e:\n",
    "        output = str(e).strip()\n",
    "    print(\"CMD: {}\\nOUT: {}\\n{}\".format(cmd, output, 10*\"-\"))\n",
    "    return output\n",
    "\n",
    "def runCommands(cmds):\n",
    "    cmds_outputs = list()\n",
    "    for cmd in cmds:\n",
    "        try:\n",
    "            output, stderr = subprocess.Popen(\n",
    "                cmd.split(\" \"),\n",
    "                stdout=subprocess.PIPE, \n",
    "                stderr=subprocess.PIPE).communicate()\n",
    "            output = output.decode(\"utf-8\").strip()\n",
    "        except Exception as e:\n",
    "            output = str(e).strip()\n",
    "        print(\"CMD: {}\\nOUT: {}\\n{}\".format(cmd, output, 10*\"-\"))\n",
    "        cmds_outputs.append(output)     \n",
    "    return cmds_outputs\n",
    "\n",
    "cmd_out  = runCommand(\"pwd\")\n",
    "cmds_out = runCommands([\"ls\", \"./trin\"])\n",
    "\n",
    "\n",
    "# In[151]:\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "\n",
    "\n",
    "# In[152]:\n",
    "\n",
    "\n",
    "# To List 5 files in your accessible bucket\n",
    "s3fs.S3FileSystem().ls(project_path)[:5]\n",
    "\n",
    "\n",
    "# ## Kaggle\n",
    "\n",
    "# In[153]:\n",
    "\n",
    "\n",
    "def downloadFromKaggle(\n",
    "    api_token = {\"username\":\"rubencg195\",\"key\":\"1a0667935c03c900bf8cc3b4538fa671\"},\n",
    "    kaggle_file_path='/home/ec2-user/.kaggle/kaggle.json',\n",
    "    zip_file_path = \"banksim1.zip\"\n",
    "    ):\n",
    "    runCommands([\n",
    "        \"rm -rf \"+str(Path(kaggle_file_path).parent),\n",
    "        \"mkdir -p \"+str(Path(kaggle_file_path).parent)\n",
    "    ])\n",
    "    with open(kaggle_file_path, 'w+') as file:\n",
    "        json.dump(api_token, file)\n",
    "    runCommands([\n",
    "        \"chmod 600 \"+kaggle_file_path,\n",
    "        \"kaggle datasets download -d ntnu-testimon/banksim1 --force\"\n",
    "    ])\n",
    "    zip_ref = zipfile.ZipFile(zip_file_path, 'r')\n",
    "    zip_ref.extractall()\n",
    "    zip_ref.close()\n",
    "    runCommand(\"ls *.csv\")\n",
    "    \n",
    "\n",
    "\n",
    "# ## Data Visualization\n",
    "# \n",
    "\n",
    "# In[154]:\n",
    "\n",
    "\n",
    "def visualize_data(data):\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"PIE CHART - FRAUD VS NON-FRAUD\" , 10*\"_ \"))\n",
    "  df_fraud= data[data['fraud']==1]\n",
    "  num_transaction_total, num_transaction_fraud = len(data), len(df_fraud)\n",
    "  num_transaction_total, num_transaction_fraud\n",
    "  print(\"Total Transactions: {} \\nTotal Fraud Transactions: {}\".format(num_transaction_total, num_transaction_fraud) )\n",
    "  percent_fraud = round((num_transaction_fraud / num_transaction_total) * 100, 2)\n",
    "  percent_safe = 100 - percent_fraud\n",
    "  percent_fraud, percent_safe\n",
    "  print(\"% Safe Transactions: {} \\n% Fraud Transactions: {}\\n\\n\".format(percent_safe, percent_fraud) ) # plotting pie chart for percentage comparision: 'fraud' vs 'safe-transaction'\n",
    "  fig1, ax1 = plt.subplots()\n",
    "  plt.title(\"Figure 1. Fraud vs Safe Transaction in Percentage\", fontsize = 20)\n",
    "  labels = ['Fraud', 'Safe Transaction']\n",
    "  sizes = [percent_fraud, percent_safe]\n",
    "  explode = (0, 0.7)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "  patches, texts, autotexts = ax1.pie(sizes,  labels=labels, autopct='%1.1f%%', shadow = True, explode=explode, startangle=130, colors = ['#ff6666', '#2d64bc'])\n",
    "  texts[0].set_fontsize(30)\n",
    "  texts[1].set_fontsize(18)\n",
    "  matplotlib.rcParams['text.color'] = 'black'\n",
    "  matplotlib.rcParams[\"font.size\"] = 30\n",
    "  plt.rcParams[\"figure.figsize\"] = [6, 6]\n",
    "  plt.show()\n",
    "  plt.savefig('PIE_CHART_FRAUD_VS_NONFRAUD_visualize_data.png')\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"COLUMN INFORMATION & PREVIEW\" , 10*\"_ \"))\n",
    "  # Extracting # of unique entires per column and their sample values\n",
    "  num_unique = []\n",
    "  sample_col_values = []\n",
    "  for col in data.columns:\n",
    "      num_unique.append(len(data[col].unique()))  # Counting number of unique values per each column\n",
    "      sample_col_values.append(data[col].unique()[:3])  # taking 3 sample values from each column   \n",
    "  # combining the sample values into a a=single string (commas-seperated)\n",
    "  # ex)  from ['hi', 'hello', 'bye']  to   'hi, hello, bye'\n",
    "  col_combined_entries = []\n",
    "  for col_entries in sample_col_values:\n",
    "      entry_string = \"\"\n",
    "      for entry in col_entries:\n",
    "          entry_string = entry_string + str(entry) + ', '\n",
    "      col_combined_entries.append(entry_string[:-2])\n",
    "  # Generating a list 'param_nature' that distinguishes features and targets\n",
    "  param_nature = []\n",
    "  for col in data.columns:\n",
    "      if col == 'fraud':\n",
    "          param_nature.append('Target')\n",
    "      else:\n",
    "          param_nature.append('Feature')\n",
    "  # Generating Table1. Parameters Overview\n",
    "  df_feature_overview = pd.DataFrame(np.transpose([param_nature, num_unique, col_combined_entries]), index = data.columns, columns = ['Parameter Nature', '# of Unique Entries', 'Sample Entries (First three values)'])\n",
    "  print(\"\\nTotal # of Values: {} \\nShape: {} \\n\\n\".format(len(data), data.shape))\n",
    "  print(df_feature_overview)\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FRAUD VS NON-FRAUD AVE. AMOUNT & PERCENTAGE\" , 10*\"_ \"))\n",
    "  df_fraud     = data.loc[data.fraud == 1] \n",
    "  df_non_fraud = data.loc[data.fraud == 0]\n",
    "  fraud_ave_amount_col          = df_fraud.groupby('category')['amount'].mean().round(2)\n",
    "  non_fraud_ave_amount_col      = df_non_fraud.groupby('category')['amount'].mean().round(2)\n",
    "  percentage_fraud_per_category = data.groupby('category')['fraud'].mean().round(3)*100\n",
    "  amount_percentage_table       = pd.concat(\n",
    "      [ fraud_ave_amount_col , non_fraud_ave_amount_col, percentage_fraud_per_category ],\n",
    "      keys=[\"Fraudulent Ave. Amount\",\"Non-Fraudulent Ave. Amount\",\"Fraud Percent(%)\"],\n",
    "      axis=1, \n",
    "      sort=False\n",
    "  ).sort_values(by=['Non-Fraudulent Ave. Amount'])\n",
    "  print(amount_percentage_table)\n",
    "  num_bins = 15                 # Number of sections where data will be segmented to be shown as a bar in the histogram. For example: The first bin is called \"0~500\"\n",
    "  tran_amount = data['amount']\n",
    "  n, bins, patches = plt.hist(tran_amount, num_bins, density = False, stacked = True, facecolor= '#f26a6a', alpha=0.5)\n",
    "  plt.close()\n",
    "  n_fraud = np.zeros(num_bins)\n",
    "  for i in range(num_bins):\n",
    "      for j in range(num_transaction_fraud):\n",
    "          if bins[i] < df_fraud['amount'].iloc[j] <= bins[i+1]:  #??????\n",
    "              n_fraud[i] += 1\n",
    "  range_amount = []\n",
    "  for i in range(num_bins):\n",
    "      lower_lim, higher_lim = str(int(bins[i])), str(int(bins[i+1]))\n",
    "      range_amount.append(\"$ \" + lower_lim + \" ~ \" + higher_lim )\n",
    "  df_hist = pd.DataFrame(index = range_amount)\n",
    "  df_hist.index.name = 'Transaction Amount[$]'\n",
    "  df_hist['# Total'] = n\n",
    "  df_hist['# Fraud'] = n_fraud\n",
    "  df_hist['# Safe'] = df_hist['# Total'] - df_hist['# Fraud']\n",
    "  df_hist['% Fraud'] = (df_hist['# Fraud'] / df_hist['# Total'] * 100).round(2)\n",
    "  df_hist['% Safe'] = (df_hist['# Safe'] / df_hist['# Total'] * 100).round(2)\n",
    "  print(df_hist)\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FREQUENCY DISTRIBUTION OF TRANSACTION AMOUNTS\" , 10*\"_ \"))  \n",
    "  fig3 = plt.figure(figsize=(16,6))\n",
    "  # Generating stacked bar-chart\n",
    "  bars_fraud = plt.bar(range(num_bins), df_hist['# Safe'], width = 0.5, color = '#00e64d')\n",
    "  bars_safe = plt.bar(range(num_bins), df_hist['# Fraud'], width = 0.5, bottom = df_hist['# Safe'], color='#ff6666')\n",
    "  # Labeling\n",
    "  plt.title(\"Figure 3. Frequency Distribution of Transaction Amount\", fontsize = 20)\n",
    "  plt.xticks(range(num_bins), range_amount, fontsize = 14)\n",
    "  plt.yticks(fontsize = 14)\n",
    "  plt.legend((bars_fraud[0], bars_safe[0]), ('Safe Transaction', 'Fraud'), loc=4, fontsize = 16)\n",
    "  plt.xlabel('Ranges of Transaction Amount', fontsize=16)\n",
    "  plt.ylabel('Number of Occurence', fontsize=16)\n",
    "  # hiding top/right border\n",
    "  ax = plt.gca()\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.spines['top'].set_visible(False)\n",
    "  x = plt.gca().xaxis\n",
    "  # rotate the tick labels for the x axis\n",
    "  for item in x.get_ticklabels():\n",
    "      item.set_rotation(50)\n",
    "  plt.show()\n",
    "  plt.savefig('FREQUENCY_DISTRIBUTION_OF_TRANSACTION_AMOUNTS_visualize_data.png')\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FRAUD PERCENTAGE AT DIFFERENT RANGES OF TRANSACTION AMOUNT\" , 10*\"_ \"))  \n",
    "  fig4 = plt.figure(figsize=(16,6))\n",
    "  # Generating stacked bar-chart\n",
    "  bars_fraud = plt.bar(range(num_bins), df_hist['% Safe'], width = 0.5, color = '#00e64d')\n",
    "  bars_safe = plt.bar(range(num_bins), df_hist['% Fraud'], width = 0.5, bottom = df_hist['% Safe'], color='#ff6666')\n",
    "  # Labeling\n",
    "  plt.title(\"Figure 4. Fraud Percentage at Different Ranges of Transaction Amount\", fontsize = 20)\n",
    "  plt.xticks(range(num_bins), range_amount, fontsize = 14)\n",
    "  plt.yticks(fontsize = 14)\n",
    "  plt.legend((bars_fraud[0], bars_safe[0]), ('Safe Transaction', 'Fraud'), loc=4, fontsize = 16)\n",
    "  plt.xlabel('Ranges of Transaction Amount', fontsize=16)\n",
    "  plt.ylabel('Percentage', fontsize=16)\n",
    "  plt.ylim(0, 100)\n",
    "  x = plt.gca().xaxis\n",
    "  # rotate the tick labels for the x axis\n",
    "  for item in x.get_ticklabels():\n",
    "      item.set_rotation(85)\n",
    "  # hiding top/right border\n",
    "  ax = plt.gca()\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.spines['top'].set_visible(False)    \n",
    "  # bar-value display\n",
    "  for bar in bars_safe:\n",
    "      plt.gca().text(bar.get_x() + bar.get_width()/2, bar.get_height() - 5, str(int(bar.get_height())) + '%', \n",
    "                  ha='center', color='w', fontsize=13, rotation = 'vertical', weight = 'bold')\n",
    "  plt.gca().text(bars_fraud[0].get_x() + bars_fraud[0].get_width()/2, bars_fraud[0].get_height() - 5, str(int(bars_fraud[0].get_height())) + '%', \n",
    "                  ha='center', color='black', fontsize=13, rotation = 'vertical', weight = 'bold')\n",
    "  plt.show()\n",
    "  plt.savefig('FRAUD_PERCENTAGE_AT_DIFFERENT_RANGES_OF_TRANSACTION_AMOUNT_visualize_data.png')\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FRAUD VS NON-FRAUD HISTOGRAM\" , 10*\"_ \"))  \n",
    "  plt.figure(figsize=(30,10))\n",
    "  sns.boxplot(x=data['category'],y=data.amount)\n",
    "  plt.title(\"Boxplot for the Amount spend in category\")\n",
    "  plt.ylim(0,4000)\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  plt.savefig('FRAUD_VS_NONFRAUD_HISTOGRAM_visualize_data.png')\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FRAUD DISTRIBUTION PER AGE\" , 10*\"_ \"))  \n",
    "  age_comparison_df = df_fraud.groupby('age')['fraud'].agg(['count']).reset_index().rename(columns={'age':'Age','count' : '# of Fraud'}).sort_values(by='# of Fraud')\n",
    "  age_df = pd.DataFrame([ [\"'0'\", \"<=18\"], [\"'1'\", \"19-25\"], [\"'2'\", \"26-35\"], [\"'3'\", \"36-45\"], [\"'4'\", \"46-55\"], [\"'5'\", \"56-65\"], [\"'6'\", \">65\"], [\"'U'\", \"Unknown\"], ], columns=[\"Age\", \"Label\"])\n",
    "  age_comparison_df = pd.merge(age_comparison_df, age_df, on=\"Age\", how=\"outer\")\n",
    "  age_comparison_df['Age'] = age_comparison_df['Age'].map(lambda x: x.strip(\"'\"))\n",
    "  age_comparison_df = age_comparison_df.sort_index(by=[\"Age\"])\n",
    "  age_comparison_df = age_comparison_df[[\"Age\", \"Label\", '# of Fraud']]\n",
    "  print(age_comparison_df)\n",
    "    \n",
    "\n",
    "def normalizing_data(data):\n",
    "  # Generate Hash Maps to be able to convert from numerical to categorical later.\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"CATEGORICAL VALUES TO NUMERICAL - HASHMAP GENERATION\" , 10*\"_ \"))\n",
    "  tmp_df = data[:]\n",
    "  col_categorical = tmp_df.select_dtypes(include= ['object']).columns\n",
    "  print( \"Features Types: \\n\\n{}\\n\\n\".format(tmp_df.dtypes) )\n",
    "  print( \"Categorical Features: {}\\n\\n\".format(col_categorical) )\n",
    "  print( \"\\nHash maps previews:\\n\" )\n",
    "  labels_hash = dict()\n",
    "  for col_name in col_categorical:                         \n",
    "    tmp_df[col_name] = tmp_df[col_name].astype('category') \n",
    "    labels_hash[col_name] = pd.DataFrame(  zip( tmp_df[col_name].cat.codes, tmp_df[col_name] ) , columns=[\"Index\", \"Label\"] ).drop_duplicates(subset=['Index'])\n",
    "    print(\"{} {} {} \\n {}\".format(10*\"_\", col_name , 10*\"_\", labels_hash[col_name].head() ) )\n",
    "  # Converting categorical entries to integers\n",
    "  tmp_df[col_categorical] = tmp_df[col_categorical].apply(lambda x: x.cat.codes)\n",
    "  # seperatign data columns and target columns\n",
    "  col_names = tmp_df.columns.tolist()\n",
    "  col_names_features = col_names[0:len(col_names)-1]\n",
    "  col_name_label     = col_names[-1]\n",
    "  # Declaring 'data-dataframe'  and 'target-dataframe'\n",
    "  X = tmp_df[col_names_features]\n",
    "  y = tmp_df[col_name_label]\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FEATURE IMPORTANCE\" , 10*\"_ \"))\n",
    "  dt = DecisionTreeClassifier()\n",
    "  print(\"\\n\\n{}\\n\\n\".format(dt))\n",
    "  dt.fit(X, y)\n",
    "  # sorted-feature importances from the preliminary decision tree \n",
    "  ds_fi = pd.Series(dt.feature_importances_ , index = col_names_features).sort_values(ascending= False)\n",
    "  # plotting feature imporance bar-graph\n",
    "  fig2 = plt.figure(figsize=(13,5))\n",
    "  # Generating stacked bar-chart\n",
    "  bars_ft = plt.bar(range(len(ds_fi)), ds_fi, width = .8, color = '#2d64bc')\n",
    "  # Labeling\n",
    "  ttl = plt.title(\"Figure 2. Feature Importances\", fontsize = 20).set_position([0.45, 1.1])\n",
    "  plt.xticks(range(len(ds_fi)), ds_fi.index, fontsize = 14)\n",
    "  # plot-dejunking\n",
    "  ax = plt.gca()\n",
    "  ax.yaxis.set_visible(False) # hide entire y axis (both ticks and labels)\n",
    "  ax.xaxis.set_ticks_position('none')  # hide only the ticks and keep the label\n",
    "  plt.xticks(rotation='vertical')\n",
    "  # hide the frame\n",
    "  for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "  # value displaying\n",
    "  rects = ax.patches  \n",
    "  labels = ds_fi.values.round(2)\n",
    "  for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height, label, ha='center', va='bottom', fontsize = 13)\n",
    "  \n",
    "  plt.show()\n",
    "  plt.savefig('FEATURE_IMPORTANCE_normalizing_data.png')\n",
    "  return labels_hash\n",
    "\n",
    "\n",
    "# # Data\n",
    "\n",
    "# ## Pre-processing\n",
    "\n",
    "# In[155]:\n",
    "\n",
    "\n",
    "def generating3DRNNInput(data):\n",
    "    tmp_df = data[:]\n",
    "    col_categorical        = tmp_df.select_dtypes(include= ['object']).columns\n",
    "    for col_name in col_categorical:                         #????\n",
    "        tmp_df[col_name] = tmp_df[col_name].astype('category') \n",
    "    tmp_df[col_categorical]  = tmp_df[col_categorical].apply(lambda x: x.cat.codes)\n",
    "    X = tmp_df.iloc[:, :-1]\n",
    "    y = tmp_df.fraud\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"VISUALIZATION BEFORE TRANSFORMATION\" , 10*\"_ \"))\n",
    "    print(\"Total Fraud vs Non-Fraud Transactions Count: \\n\\n{}\\n\".format(y.value_counts()))\n",
    "    print(\"Number of customers: \",  X[\"customer\"].nunique() )\n",
    "    print(\"Ratio of positive frauds vs total dataset: {:0.2f}%\".format( ( y[y==1].count() /len(X)) *100  ))\n",
    "    # The RNNs requiere various a 3D input of groups of less of 300 samples per group for better performance. One option is to divide the datasets per day. \n",
    "    # Even by dividing per day, each day has more than 7K data points per data group.\n",
    "    mean_samples_per_customer = X[\"customer\"].value_counts().mean()\n",
    "    max_samples_per_customer = X[\"customer\"].value_counts().max()\n",
    "    print(\"\\nTransactions per customer.\\n\\tMean: {:0.1f}\\n\\tMax:  {:0.0f} \\n\\tNumber of Batches Using Max Amount as Fixed Size: {:0.1f} ~ {}\\n\\tNumber of Batches Using Mean Amount as Fixed Size: {:0.1f} ~ {}\".format(\n",
    "        mean_samples_per_customer, \n",
    "        max_samples_per_customer, \n",
    "        len(X) / max_samples_per_customer ,\n",
    "        math.ceil(len(X) / max_samples_per_customer ),\n",
    "        len(X) / mean_samples_per_customer ,\n",
    "        math.ceil(len(X) / mean_samples_per_customer )\n",
    "    ))\n",
    "    mean_samples_per_day = X[\"step\" ].value_counts().mean()\n",
    "    max_samples_per_day = X[\"step\"].value_counts().max()\n",
    "    print(\"\\n\\nSamples per Step (day): \\n\\tMean: {:0.0f} \\n\\tMax: {} \\n\\tNumber of Batches Using Max Amount as Fixed Size: {:0.1f} ~ {}\\n\\tNumber of Batches Using Mean Amount as Fixed Size: {:0.1f} ~ {}\".format(\n",
    "        mean_samples_per_day, max_samples_per_day, \n",
    "        len(X) / max_samples_per_day ,\n",
    "        math.ceil(len(X) / max_samples_per_day ),\n",
    "        len(X) / mean_samples_per_day ,\n",
    "        math.ceil(len(X) / mean_samples_per_day )\n",
    "    ))\n",
    "\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"GROUPING TRANSACTIONS BY CUSTOMER ID\" , 10*\"_ \"))\n",
    "    customer_batches = dict()\n",
    "    count = 0\n",
    "    for x in tmp_df.groupby([\"customer\"]):\n",
    "        customer_batches[x[0]] = x[1] \n",
    "    min_trans_per_cust = np.min( [ customer_batches[i].shape[0] for i in customer_batches]  ) \n",
    "    mean_trans_per_cust = np.mean( [ customer_batches[i].shape[0] for i in customer_batches]  ) \n",
    "    max_trans_per_cust = np.max( [ customer_batches[i].shape[0] for i in customer_batches]  ) \n",
    "    n_features =  tmp_df.shape[1]\n",
    "    print(\"LEN: \", len(customer_batches))\n",
    "    print(\"# Feaures: \", n_features )\n",
    "    print(\"MIN TRANS PER CUST: \", min_trans_per_cust )\n",
    "    print(\"MEAN TRANS PER CUST: \", mean_trans_per_cust, \" ~ \", math.ceil(mean_trans_per_cust) )\n",
    "    print(\"MAX TRANS PER CUST: \", max_trans_per_cust)\n",
    "    print(\"EXAMPLE: \\n\\n\", customer_batches[0] )\n",
    "\n",
    "    # See how many groups of customer transactions are above the average size of 145\n",
    "    print(\"\\n\\nSCATTER PLOT SHOWING SIZES OF BATCHES GROUPED BY CUSTOMER ID: \\n\\n\" )\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_axes([0,0,1,1])\n",
    "    ax.scatter(range(4112), [ customer_batches[i].shape[0] for i in customer_batches] , color='r')\n",
    "    ax.plot([0, 4112], [145, 145], \"--\")\n",
    "    ax.set_xlabel('Batch Index')\n",
    "    ax.set_ylabel('Batch Size')\n",
    "    ax.set_title('Customer Batches Sizes')\n",
    "    plt.show()\n",
    "    plt.savefig('SCATTER_PLOT_SHOWING_SIZES_OF_BATCHES_GROUPED_BY_CUSTOMER_ID_generating3DRNNInput.png')\n",
    "\n",
    "    print(\"\\n\\nBOX PLOT SHOWING MEAN SIZE OF BATCHES GROUPED BY CUSTOMER ID: \\n\\n\" )\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.set_title('Batch Sizes')\n",
    "    ax1.boxplot([ customer_batches[i].shape[0] for i in customer_batches],  vert=False)\n",
    "    plt.show()\n",
    "    plt.savefig('BOX_PLOT_SHOWING_SIZES_OF_BATCHES_GROUPED_BY_CUSTOMER_ID_generating3DRNNInput.png')\n",
    "\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"GENERATING 3D INPUT WITH BATCHES OF SIZE \"+str(max_trans_per_cust) , 10*\"_ \"))\n",
    "    # Full 3D Array as an input for the LSTM\n",
    "    #   empty_padding_value = 0\n",
    "    np_customer_batches = list()\n",
    "    total_empty_rows_added = 0\n",
    "    for k in customer_batches:\n",
    "        empty_rows_to_add =  max_trans_per_cust - customer_batches[k].shape[0]\n",
    "        z = np.full( ( empty_rows_to_add , n_features ), empty_padding_value )\n",
    "        np_customer_batches.append( np.r_[ customer_batches[k].values , z] )\n",
    "        total_empty_rows_added += empty_rows_to_add\n",
    "    np_customer_batches_3d = np.array(np_customer_batches)\n",
    "    mean_frauds_per_batch       = np.mean( [ len(b[-1][b[-1] == 1 ]) for b in np_customer_batches_3d ]  )  \n",
    "    percentage_frauds_per_batch = np.mean( [ len(b[-1][b[-1] == 1 ]) / max_trans_per_cust for b in np_customer_batches_3d ]  )  * 100\n",
    "    print(\n",
    "      \"\"\"\n",
    "      The batches are separated by customer id. To be able to use the batches as input for the RNN, \n",
    "      it needs to have a static size. That is why the batch size is defined by the max number of \n",
    "      transactions done by the customers ({}). If one of the customers have done less transactions,\n",
    "      the rest of the empty space is filled with {} values. The final array size is {}.\\n\\n\"\"\".format(max_trans_per_cust, empty_padding_value, np_customer_batches_3d.shape ),\n",
    "      \"\\nTotal Empty Rows Added: \", total_empty_rows_added, \n",
    "      \"\\nPercentage of Empty Rows Added Compared to Total # of Data Points: %\", np.round(total_empty_rows_added / (max_trans_per_cust * len(np_customer_batches) ) * 100 , 2), \n",
    "      \"\\nNew Shape: \", np_customer_batches_3d.shape,\n",
    "      \"\\nMean of frauds per batch: \", np.round(mean_frauds_per_batch, 5),\n",
    "      \"\\nPercentage of frauds per batch: \", np.round(percentage_frauds_per_batch, 5),\n",
    "    )\n",
    "\n",
    "    np_customer_batches = list()\n",
    "    np_left_over_transactions = np.empty(shape=[0, n_features])\n",
    "    total_empty_rows_added = 0\n",
    "    mean_trans_per_cust = math.ceil(mean_trans_per_cust)\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"GENERATING 3D INPUT WITH BATCHES OF SIZE \"+str(mean_trans_per_cust) , 10*\"_ \"))\n",
    "    for k in customer_batches:\n",
    "        if( mean_trans_per_cust > customer_batches[k].shape[0] ):\n",
    "            empty_rows_to_add =  mean_trans_per_cust - customer_batches[k].shape[0]\n",
    "            z = np.full( ( empty_rows_to_add , n_features ) , empty_padding_value )\n",
    "            np_customer_batches.append( np.r_[ customer_batches[k].values , z] )\n",
    "            total_empty_rows_added += empty_rows_to_add\n",
    "        else:\n",
    "            np_customer_batches.append( np.array(customer_batches[k][0:mean_trans_per_cust].values) ) \n",
    "            np_left_over_transactions = np.r_[ np_left_over_transactions , customer_batches[k][mean_trans_per_cust:].values ]  #axis 0 to append vertically.\n",
    "    left_over_n_batches = math.ceil( len(np_left_over_transactions) / mean_trans_per_cust )\n",
    "    left_over_z = np.full( ( (left_over_n_batches * mean_trans_per_cust ) - len(np_left_over_transactions)  , n_features ),  empty_padding_value )\n",
    "    np_left_over_transactions    = np.r_[ np_left_over_transactions , left_over_z ] \n",
    "    np_left_over_transactions_3d = np.reshape(np_left_over_transactions, (left_over_n_batches, mean_trans_per_cust, n_features )  )\n",
    "    np_shifted_customer_batches_3d       = np.r_[ np.array(np_customer_batches) , np_left_over_transactions_3d ] \n",
    "    total_empty_rows = total_empty_rows_added + len(left_over_z)\n",
    "    mean_frauds_per_batch       = np.mean( [ len(b[-1][b[-1] == 1 ]) for b in np_shifted_customer_batches_3d ]  )  \n",
    "    percentage_frauds_per_batch = np.mean( [ len(b[-1][b[-1] == 1 ]) / mean_trans_per_cust for b in np_shifted_customer_batches_3d ]  )  * 100\n",
    "    print(\n",
    "      \"\"\"\n",
    "      The batches are separated by customer id. To be able to use the batches as input for the RNN, \n",
    "      it needs to have a static size. That is why the batch size is defined by the average number of \n",
    "      transactions done by the customers ({}). If one of the customers have done less transactions,\n",
    "      the rest of the empty space is filled with {} values. The final array size is {}. The difference\n",
    "      between this new more compacted version than previous which uses the max amount of transactions\n",
    "      per customers is that if a customer has more than the average number of transactions, these \n",
    "      transactions are saved in a separate array called left_overs. The left overs are then shaped as\n",
    "      a 3D array and appended to the main array. The problem with this array which is more efficient in \n",
    "      space and has less empty rows is that the mayority of batches are arranged by customer ID but the\n",
    "      last batches are in disorder, having transactions from many customers.\\n\\n\"\"\".format(mean_trans_per_cust, empty_padding_value, np_shifted_customer_batches_3d.shape ),\n",
    "      \"\\nTotal Empty Rows Added: \", total_empty_rows_added, \n",
    "      \"\\n% Empty Rows Added: %\", (total_empty_rows_added / (mean_trans_per_cust * len(np_customer_batches) ) * 100 ), \n",
    "      \"\\nNew Shape: \", np.array(np_customer_batches).shape,\n",
    "      \"\\nLeft overs: \", np_left_over_transactions.shape,\n",
    "      \"\\nLeft overs %: \", np.round( len(np_left_over_transactions)  / (mean_trans_per_cust * len(np_customer_batches) ) * 100, 1 ),\n",
    "      \"\\nLeft overs new Shape: \", np_left_over_transactions_3d.shape,\n",
    "      \"\\nLeft overs empty rows: \", len(left_over_z),\n",
    "      \"\\nLeft overs empty rows percentage (%) over total dataset: \", np.round( len(left_over_z)  / (mean_trans_per_cust * len(np_customer_batches) ) * 100, 4 ),\n",
    "      \"\\nTotal Empty # Rows and %: \", total_empty_rows, \" - \", ( total_empty_rows / (mean_trans_per_cust * len(np_customer_batches) ) ) * 100,\n",
    "      \"\\nMean of frauds per batch: \", mean_frauds_per_batch,\n",
    "      \"\\nPercentage of frauds per batch: \", percentage_frauds_per_batch,\n",
    "      \"\\nFinal 3D Array Shape (Customer Batches + Left overs): \", np_shifted_customer_batches_3d.shape,\n",
    "    )\n",
    "    return np_customer_batches_3d, np_shifted_customer_batches_3d\n",
    "\n",
    "\n",
    "# ## Generating New Features\n",
    "\n",
    "# In[156]:\n",
    "\n",
    "\n",
    "def generateNewFeatures(data):\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"GENERATING NEW FEATURES\" , 10*\"_ \"))\n",
    "    print(\n",
    "    \"\"\"\n",
    "        The following features will created using the original Data. \n",
    "        The data generated is calculated inside each batch or group\n",
    "        or transactions grouped by custmer ID. Each calculation takes\n",
    "        all the data points before the current transaction in which the\n",
    "        loop index is currently located.\n",
    "\n",
    "        \\t Current day number of transactions  - \"curr_day_tr_n\"\n",
    "        \\t Average transaction amount per day  - \"ave_tr_p_day_amount\"\n",
    "        \\t Total average transaction amount \n",
    "        \\t From the beggining to current time  - \"tot_ave_tr_amount\"\n",
    "        \\t Is the merchant new?                - \"is_mer_new\"\n",
    "        \\t What is the common transaction type - \"com_tr_type\"\n",
    "        \\t What is the common merchant ID      - \"com_mer\"\n",
    "        \\n\\n\"\"\".format(),\n",
    "    )\n",
    "    # Customer ID column will be disposed. The ID is the same as the batch position in the array\n",
    "    columns=[\n",
    "        \"day\", \"age\", \"gender\", \"merchant\", \"category\", \"amount\", \n",
    "        \"curr_day_tr_n\",\"ave_tr_p_day_amount\", \"tot_ave_tr_amount\", \"is_mer_new\",\"com_tr_type\", \"com_mer\",\n",
    "        \"fraud\"\n",
    "  ]       #Original Cols : step ,customer , age, gender , merchant, category, amount , fraud \n",
    "    new_data = list()\n",
    "    start_time = time.time()\n",
    "    bar = progressbar.ProgressBar(max_value=len(data)) \n",
    "    print(30*\"_ \", \"\\n\\n\")\n",
    "    for b_i, b in enumerate( data ):\n",
    "        new_batch = pd.DataFrame(columns=columns)\n",
    "        for t_i, trans in enumerate( b ) :\n",
    "            step_col, merchant_col, cat_col, amount_col, is_fraud_col = 0, 4, 5, 6, 7\n",
    "            current_merchant = trans[merchant_col] # Merchant column\n",
    "            current_day      = trans[step_col]     # Day(step) column\n",
    "            current_cat      = trans[cat_col]     # Trans type column\n",
    "            is_new_merchant           = 0 if len( b[ :t_i, merchant_col ][ b[ :t_i, merchant_col ] == trans[merchant_col] ] ) > 0 else 1\n",
    "            ave_trans_amount          = np.around(np.mean( b[:t_i, amount_col ] ), 2 )\n",
    "            most_common_trans_type    =  np.bincount( b[:t_i,  cat_col ][b[:t_i, cat_col ] > 0].astype(int) ).argmax()  if  len(np.bincount( b[:t_i, cat_col ][b[:t_i, cat_col ] > 0].astype(int) )) > 0 else -1\n",
    "            most_common_merchant      =  np.bincount( b[:t_i,  merchant_col ][b[:t_i, merchant_col ] > 0].astype(int) ).argmax()  if  len(np.bincount( b[:t_i, merchant_col ][b[:t_i, merchant_col ] > 0].astype(int) )) > 0 else -1\n",
    "            ave_n_trans_per_day       =  np.round(pd.DataFrame(b[:t_i, [step_col, amount_col]][ b[:t_i, step_col ] != -1 ], index=None, columns=None).groupby(by=0).mean().reset_index().values[:, 1].mean() , 2)\n",
    "            n_trans_this_day          =  len( b[ :t_i + 1, step_col ][ b[ :t_i + 1, step_col ] == trans[step_col] ] )\n",
    "            ave_amount_for_curr_trans_type =  np.around(np.mean( b[:t_i+1, amount_col ][ b[:t_i+1, cat_col ] == trans[cat_col] ] ), 2 )\n",
    "            tr_data = {\n",
    "            \"day\": current_day, \"age\": trans[2], \"gender\": trans[3], \"merchant\": current_merchant, \"category\": current_cat, \"amount\" : trans[amount_col], \n",
    "            \"curr_day_tr_n\" : n_trans_this_day ,\"ave_tr_p_day_amount\": ave_n_trans_per_day, \"tot_ave_tr_amount\": ave_trans_amount, \"is_mer_new\": is_new_merchant, \"com_tr_type\" : most_common_trans_type, \"com_mer\": most_common_merchant,\n",
    "            \"fraud\" : trans[is_fraud_col]\n",
    "            }\n",
    "            new_batch = new_batch.append( tr_data , ignore_index=True)\n",
    "            # print(new_batch)\n",
    "            # break\n",
    "        new_data.append(new_batch.fillna(empty_padding_value).values)\n",
    "        bar.update(b_i)\n",
    "    new_data = np.array(new_data)\n",
    "    delta_time = time.time() - start_time\n",
    "    print(\"--- {:0.2f} s seconds or {:0.2f} minutes ---\".format(delta_time, delta_time/60 ))\n",
    "    print(new_data.shape)\n",
    "    print(new_data[0])\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# ## Sliding Window\n",
    "\n",
    "# In[157]:\n",
    "\n",
    "\n",
    "def separateInBatches(customer_batches, min_batch_size=15):\n",
    "    #print(customer_batches.shape)\n",
    "    #min_batch_size = np.min([ pd.DataFrame(cb)[ pd.DataFrame(cb)[0] != -1 ].shape[0] for cb in customer_batches ])\n",
    "    #max_batch_size = np.max([ pd.DataFrame(cb)[ pd.DataFrame(cb)[0] != -1 ].shape[0] for cb in customer_batches ])\n",
    "    #print(\"MIN \", min_batch_size, \" MAX \", max_batch_size)\n",
    "    #padding_value = 0 \n",
    "    padding_value                = empty_padding_value\n",
    "    column_to_check              = 0\n",
    "    new_customer_batches         = list()\n",
    "    new_labels                   = list()\n",
    "    new_grouped_customer_batches = list()\n",
    "    new_grouped_customer_labels  = list()\n",
    "    data_batches  = customer_batches[:, :, :-1]\n",
    "    label_batches = customer_batches[:, :, -1]\n",
    "    columns=[\n",
    "        \"day\", \"age\", \"gender\", \"merchant\", \"category\", \"amount\", \n",
    "        \"curr_day_tr_n\",\"ave_tr_p_day_amount\", \"tot_ave_tr_amount\", \"is_mer_new\",\"com_tr_type\", \"com_mer\",\n",
    "#         \"fraud\"\n",
    "    ]  \n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"SEPARATING IN BATCHES OF \"+str(min_batch_size) , 10*\"_ \"))\n",
    "    print(\"\"\"\n",
    "    In the following procedure, the batches separaeted by customer and\n",
    "    generated in the previous function are iterated. Each transaction\n",
    "    that has been padded with value of '{}' will be deleted. new batches\n",
    "    of size '{}' will be generated using sliding window through each \n",
    "    transaction in every customer batch. Padding will be added for batches \n",
    "    with less transactions than the min amount '{}'.\n",
    "    Input Shape:                 {}\n",
    "    Data Shape:                  {}\n",
    "    Label Len:                   {}\n",
    "    Padding Value:               {}\n",
    "    Column to check for \n",
    "    padding values to delete     {}\n",
    "    transaction in the new batch\n",
    "    \"\"\".format(\n",
    "        padding_value, min_batch_size, min_batch_size,\n",
    "        customer_batches.shape, data_batches.shape, label_batches.shape,\n",
    "        padding_value , column_to_check ))\n",
    "    \n",
    "    bar           = progressbar.ProgressBar(max_value=len(customer_batches))\n",
    "    cb_count = 0\n",
    "    skipped_rows = 0\n",
    "    for cb_i, cb in enumerate(data_batches):\n",
    "        cb_df = pd.DataFrame(cb)\n",
    "#         cb_df = cb_df[cb_df[column_to_check] != padding_value ]\n",
    "        grouped_customer_batches = list()\n",
    "        grouped_customer_labels = list()\n",
    "        for i, trans in enumerate(cb_df.values):\n",
    "            label      =  label_batches[cb_i, i]\n",
    "#             print(\"{}. Transactions Shape \\n\\n{}\\n\\n labels shape {}\".format( i, trans, label ))\n",
    "            if(label == -1):\n",
    "                skipped_rows +=1\n",
    "                continue\n",
    "            init_index           = 0 if i+1 <= min_batch_size else i+1 - min_batch_size\n",
    "            trans_before_current = cb_df[ init_index:i+1 ]\n",
    "            n_features           = cb_df.shape[1]\n",
    "            empty_rows_to_add    = 0 if min_batch_size <= len(trans_before_current) else min_batch_size - len(trans_before_current)\n",
    "            z = np.full( ( empty_rows_to_add , n_features ), padding_value )\n",
    "            new_batch = pd.DataFrame( np.r_[ z, trans_before_current.values ] )\n",
    "            #print(i, \" trans before \", len(trans_before_current),\" empty_rows_to_add \", empty_rows_to_add, \" original shape \", cb_df.shape, \" new shape \",new_batch.shape )\n",
    "            #print(new_batch.tail() ,  \"\\n____________\" )\n",
    "            new_batch = np.array(new_batch.values)\n",
    "            new_customer_batches.append( new_batch )\n",
    "            new_labels.append(label)\n",
    "            grouped_customer_batches.append(new_batch  )\n",
    "            grouped_customer_labels.append(label)\n",
    "#         break\n",
    "        new_grouped_customer_batches.append( np.array(grouped_customer_batches ) )\n",
    "        new_grouped_customer_labels.append(  np.array(grouped_customer_labels  ) )\n",
    "        #print(cb_df.shape, \"\\n\", cb_df.tail(), \"\\n____________\" )\n",
    "        #break\n",
    "        cb_count += 1\n",
    "        bar.update(cb_count)\n",
    "    new_customer_batches         = np.array(new_customer_batches)\n",
    "    new_labels                   = np.array(new_labels)\n",
    "    new_grouped_customer_batches = np.array( new_grouped_customer_batches)\n",
    "    new_grouped_customer_labels  = np.array( new_grouped_customer_labels)    \n",
    "    \n",
    "    X         = new_customer_batches \n",
    "    grouped_X = new_grouped_customer_batches\n",
    "    y         = new_labels \n",
    "    grouped_y = new_grouped_customer_labels\n",
    "    \n",
    "    #REPLACE PADDING WITH 0\n",
    "#     X[ X == padding_value ] = 0\n",
    "#     y[ y == padding_value ] = 0\n",
    "#     grouped_X[ grouped_X == padding_value ] = 0\n",
    "#     grouped_y[ grouped_y == padding_value ] = 0\n",
    "    \n",
    "    len_per_cust_group    = [len(gc) for gc in grouped_X ]\n",
    "    frauds_per_cust_group = [len(fgc[fgc==1]) for fgc in grouped_y ]\n",
    "    print(\"\"\"\n",
    "    X Shape: {}\n",
    "    y Shape: {}\n",
    "    X Grouped/Cust Shape: ( {} , ~ MIN:{}|AVE:|{}|MAX:{} , {}  )\n",
    "    y Grouped/Cust Shape: ({}, ~ MIN:{}|AVE:|{}|MAX:{} )\n",
    "    \n",
    "    # TRANSACTIONS GROUPS PER CUSTOMER\n",
    "    Min                      : {} \n",
    "    Max                      : {} \n",
    "    Ave                      : {}\n",
    "    Total Transaction Groups : {}\n",
    "    # Frauds & %             : {}  - {}%\n",
    "    # Non-Fraud & %          : {}  - {}%\n",
    "    % Frauds per customer    : {}%\n",
    "    Cust Id with most fraud  : \n",
    "        ID  : {} #-Frauds: {} of #Trans {} \n",
    "        Note: The id is not the original. It has to be transformed using the label_hash.\n",
    "    Skipped rows due to having all -1: {}\n",
    "    \"\"\".format(\n",
    "        X.shape,\n",
    "        y.shape,\n",
    "        len(grouped_X), min(len_per_cust_group), np.round(np.average(len_per_cust_group), 2), max(len_per_cust_group), n_features,\n",
    "        len(grouped_y), min(len_per_cust_group), np.round(np.average(len_per_cust_group), 2), max(len_per_cust_group), \n",
    "        min(len_per_cust_group), \n",
    "        max(len_per_cust_group), \n",
    "        np.round(np.average(len_per_cust_group), 2),\n",
    "        len(X),\n",
    "        len(y[y==1]), np.round(len(y[y==1])*100/len(y), 2),\n",
    "        len(y[y==0]), np.round(len(y[y==0])*100/len(y), 2),\n",
    "        np.round(np.average(frauds_per_cust_group), 2),\n",
    "        np.argmax(frauds_per_cust_group), frauds_per_cust_group[np.argmax(frauds_per_cust_group)], len(grouped_y[np.argmax(frauds_per_cust_group)]), \n",
    "        skipped_rows\n",
    "    ))\n",
    "    \n",
    "    print(\"Tail Sample of X \\n\\n{}\\n\")\n",
    "    print(pd.DataFrame(X[0], \n",
    "#                        columns=columns, index=None\n",
    "                      ).tail())\n",
    "    print(\"Sample of y \\n\\n{}\\n\".format( y[0] ))\n",
    "    return X, grouped_X, y , grouped_y\n",
    "\n",
    "\n",
    "# ## Data Split\n",
    "# \n",
    "# TRAIN 60% & TEST 20% & VAL 20%\n",
    "\n",
    "# In[158]:\n",
    "\n",
    "\n",
    "def separateLabel(data):\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"SEPARATING X & y FOR TRAINING\" , 10*\"_ \"))\n",
    "    X = data[:, :, 0:-1]\n",
    "    y = data[:, :, -1]\n",
    "    #Replacing -1 values to 0\n",
    "#     y[ data[:, :, -1] == -1] = 0\n",
    "    print(\"X Shape: {} Y Shape: {}\".format(X.shape, y.shape))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# In[159]:\n",
    "\n",
    "\n",
    "def separatingTrainTest(X, y, test_size=0.2, val_size=0.2):\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"SEPARATING TEST & TRAIN\" , 10*\"_ \"))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size+val_size, \n",
    "        random_state=1,\n",
    "        shuffle=True,\n",
    "        stratify=None#y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, \n",
    "        test_size=val_size, \n",
    "        random_state=1,\n",
    "        shuffle=True,\n",
    "        stratify=None#y\n",
    "    )\n",
    "\n",
    "    print(\"\"\"\n",
    "    X-TRAIN Shape: {}\n",
    "    Y-TRAIN Shape: {} #-Frauds: {} #-Non-Frauds: {}\n",
    "    X-TEST Shape:  {}\n",
    "    Y-YEST Shape:  {} #-Frauds: {} #-Non-Frauds: {}\n",
    "    Total-#-Frauds: {} Total-#-Non-Frauds: {}\n",
    "    \\n\"\"\".format(\n",
    "      X_train.shape, \n",
    "      y_train.shape, np.count_nonzero( y_train == 1 ), np.count_nonzero( y_train == 0 ),\n",
    "      X_test.shape, \n",
    "      y_test.shape,  np.count_nonzero( y_test  == 1 ), np.count_nonzero( y_test  == 0 ),\n",
    "      np.count_nonzero( y  == 1 ), np.count_nonzero( y  == 0 )\n",
    "    ))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_val, y_val\n",
    "\n",
    "\n",
    "# **10 K-Fold**\n",
    "\n",
    "# ## Normalized data\n",
    "\n",
    "# In[160]:\n",
    "\n",
    "\n",
    "def normalize3DInput(data, filename=\"scaler.data\"):\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"SEPARATING TEST & TRAIN\" , 10*\"_ \"))\n",
    "    n_batches        = data.shape[0]\n",
    "    batch_size       = data.shape[1]\n",
    "    n_features       = data.shape[2]\n",
    "    tmp_data         = data.reshape( (n_batches * batch_size, n_features ) )\n",
    "    print(\"Converting 3D to 2D for easy processing. Batch Sample: \\n\\n {} \\n\\n Original Array Shape: {}. Temporary array with shape: {}\\n\".format( data[0], data.shape, tmp_data.shape )) #(4112, 265, 12)\n",
    "\n",
    "    min_max_scaler    = MinMaxScaler()\n",
    "    data_norm         = min_max_scaler.fit_transform(tmp_data)                      # ROBUST SCALER ANOTHER OPTION\n",
    "    scaler_max        = min_max_scaler.data_max_                      \n",
    "    scaler_min        = min_max_scaler.data_min_                      \n",
    "    scaler_scale      = min_max_scaler.scale_                      \n",
    "    scaler_data_range = min_max_scaler.data_range_    \n",
    "    scaler_params     = min_max_scaler.get_params(deep=True)\n",
    "    data_norm         = data_norm.reshape( (n_batches, batch_size, n_features) )\n",
    "    \n",
    "    print(\"\"\"\n",
    "    SCALER INFORMATION\n",
    "    MAX:    {}\n",
    "    MIN:    {}\n",
    "    SCALE:  {}\n",
    "    RANGE:  {}\n",
    "    PARAMS: {}\n",
    "    Data Normalized and reshaped to a 3D array. \n",
    "    Current Shape: {} \n",
    "    Batch Sample: \n",
    "    \n",
    "    {}\n",
    "    \n",
    "    Saving scaler to file: {}\n",
    "    \"\"\".format( \n",
    "        scaler_max,                             \n",
    "        scaler_min,                    \n",
    "        scaler_scale,                        \n",
    "        scaler_data_range,   \n",
    "        scaler_params,\n",
    "        data_norm.shape, \n",
    "        data_norm[0] ,\n",
    "        filename\n",
    "    )) #(4112, 265, 12)\n",
    "    joblib.dump(min_max_scaler, filename)\n",
    "    return data_norm\n",
    "\n",
    "\n",
    "# ## Generate Data & Read Data\n",
    "\n",
    "# In[161]:\n",
    "\n",
    "\n",
    "def read_data(input_file_path):\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"IMPORT DATA FROM CSV\" , 10*\"_ \"))\n",
    "    data = pd.read_csv(input_file_path)\n",
    "    print(\"Deleting the columns 'zipcodeOri','zipMerchant' because all the fields are equal.\\n\\n\")\n",
    "    del data['zipcodeOri']\n",
    "    del data['zipMerchant']\n",
    "    print(\"Data Shape: {} \\n\\nPreview: \\n\\n {} \\n\\n Data Information: \\n\".format( data.shape, data.head() ))\n",
    "    print(\"\\n{}\\nDoes it has null values? {}\".format(data.info(), data.isnull().values.any() ))\n",
    "    return data\n",
    "\n",
    "def readLocally():\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \" READ DATA LOCALLY \" , 10*\"_ \"))\n",
    "    \n",
    "    X_train     = pickle.load( open( \"X_train.data\"       , \"rb\" ) ) \n",
    "    y_train     = pickle.load( open( \"y_train.data\"       , \"rb\" ) )\n",
    "    \n",
    "    X_test      = pickle.load( open( \"X_test.data\"        , \"rb\" ) )\n",
    "    y_test      = pickle.load( open( \"y_test.data\"        , \"rb\" ) )\n",
    "    \n",
    "    X_val      = pickle.load( open( \"X_val.data\"         , \"rb\" ) )\n",
    "    y_val      = pickle.load( open( \"y_val.data\"         , \"rb\" ) )\n",
    "    \n",
    "    labels_hash = pickle.load( open( \"labels_hash.data\"   , \"rb\" ) )\n",
    "    \n",
    "    print(\"\"\"\\n\\nSHAPES & KEYS:\n",
    "    X_train          : {}\n",
    "    y_train          : {}\n",
    "    X_test           : {}\n",
    "    y_test           : {}\n",
    "    X_val            : {}\n",
    "    y_val            : {}\n",
    "    labels_hash Keys : {}\n",
    "    \"\"\".format(\n",
    "        X_train.shape, y_train.shape, \n",
    "        X_test.shape,  y_test.shape, \n",
    "        X_val.shape,  y_val.shape, \n",
    "        labels_hash.keys() \n",
    "    ))\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val, labels_hash \n",
    "\n",
    "def saveToCloud( \n",
    "    X_train, X_test, y_train, y_test, X_val, y_val, history, rnn, model_name, \n",
    "    home_dir=\"/home/ec2-user/SageMaker/\"):\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \" SAVE TO CLOUD \" , 10*\"_ \"))\n",
    "    \n",
    "    img_bucket_path   = project_path+\"/images\"\n",
    "    data_bucket_path  = project_path+\"/data\"\n",
    "    model_bucket_path = project_path+\"/models\"\n",
    "    \n",
    "    print(\"\\n\\nCOPYING IMAGES FILES ({})\\n\\n\".format(img_bucket_path))\n",
    "    runCommand('aws s3 cp {} {} --recursive --exclude=\"*\" --include=\"{}\"'.format(home_dir, img_bucket_path, \"*.png\"))\n",
    "    print(\"\\n\\nCOPYING DATA FILES ({})\\n\\n\".format(data_bucket_path))\n",
    "    runCommand('aws s3 cp {} {} --recursive --exclude=\"*\" --include=\"{}\"'.format(home_dir, data_bucket_path, \"*.data\"))\n",
    "    print(\"\\n\\nCOPYING MODEL FILES ({})\\n\\n\".format(model_bucket_path))\n",
    "    runCommand('aws s3 cp {} {} --recursive --exclude=\"*\" --include=\"{}\"'.format(home_dir, model_bucket_path, \"*.h5\"))\n",
    "    \n",
    "    print(\"\\n\\nImages Directory\\n\\n\")\n",
    "    try: \n",
    "        print( s3fs.S3FileSystem().ls(img_bucket_path) ); \n",
    "    except: \n",
    "        print(\"No Files In Folder.\")\n",
    "    print(\"\\n\\nData Directory\\n\\n\")\n",
    "    try: \n",
    "        print( s3fs.S3FileSystem().ls(data_bucket_path) ); \n",
    "    except: \n",
    "        print(\"No Files In Folder.\")\n",
    "    print(\"\\n\\nModel Directory\\n\\n\")\n",
    "    try: \n",
    "        print( s3fs.S3FileSystem().ls(model_bucket_path) );\n",
    "    except: \n",
    "        print(\"No Files In Folder.\")\n",
    "\n",
    "def readDataFromCloud():\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \" READ DATA FROM CLOUD \" , 10*\"_ \"))\n",
    "    \n",
    "    data_bucket_path = project_path+\"/data\"\n",
    "    print(\"\\n\\nDownloading data from: \"+data_bucket_path+\"\\n\\n\")\n",
    "#     get_ipython().system('aws s3 cp {data_bucket_path}/X_train.data X_train.data')\n",
    "#     get_ipython().system('aws s3 cp {data_bucket_path}/X_test.data  X_test.data')\n",
    "#     get_ipython().system('aws s3 cp {data_bucket_path}/y_train.data y_train.data')\n",
    "#     get_ipython().system('aws s3 cp {data_bucket_path}/y_test.data  y_test.data')\n",
    "#     get_ipython().system('aws s3 cp {data_bucket_path}/labels_hash.data  labels_hash.data')\n",
    "    \n",
    "    print(\"\\n\\nList the data files.\\n\\n\")\n",
    "#     get_ipython().system('pwd')\n",
    "#     get_ipython().system('ls -la *.data')\n",
    "\n",
    "#     root_path   = \"/content/gdrive/My Drive/Verafin/\"\n",
    "    root_path   = \"/home/ec2-user/SageMaker/\"\n",
    "    \n",
    "    X_train     = pickle.load( open( \"X_train.data\"       , \"rb\" ) ) \n",
    "    y_train     = pickle.load( open( \"y_train.data\"       , \"rb\" ) )\n",
    "    \n",
    "    X_test      = pickle.load( open( \"X_test.data\"        , \"rb\" ) )\n",
    "    y_test      = pickle.load( open( \"y_test.data\"        , \"rb\" ) )\n",
    "    \n",
    "    X_val      = pickle.load( open( \"X_val.data\"         , \"rb\" ) )\n",
    "    y_val      = pickle.load( open( \"y_val.data\"         , \"rb\" ) )\n",
    "    \n",
    "    labels_hash = pickle.load( open( \"labels_hash.data\"   , \"rb\" ) )\n",
    "    \n",
    "    print(\"\"\"\\n\\nSHAPES & KEYS:\n",
    "    X_train          : {}\n",
    "    y_train          : {}\n",
    "    X_test           : {}\n",
    "    y_test           : {}\n",
    "    X_val            : {}\n",
    "    y_val            : {}\n",
    "    labels_hash Keys : {}\n",
    "    \"\"\".format(\n",
    "        X_train.shape, y_train.shape, \n",
    "        X_test.shape,  y_test.shape, \n",
    "        X_val.shape,  y_val.shape, \n",
    "        labels_hash.keys() \n",
    "    ))\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val, labels_hash \n",
    "\n",
    "\n",
    "# # Model\n",
    "\n",
    "# ## Base Model\n",
    "\n",
    "# In[162]:\n",
    "\n",
    "\n",
    "class MLModel():\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  def visualize_data(self, data):\n",
    "    pass\n",
    "  def preprocess(self, data):\n",
    "    X, y = None, None\n",
    "    return X, y\n",
    "  def create_model(self):\n",
    "    pass\n",
    "  def train(self, X, y):\n",
    "    pass\n",
    "  def evaluate( self, X_test, y_test ):\n",
    "    _, test_acc = self.model.evaluate( X_test, y_test , verbose=1)\n",
    "    print('Test Accuracy: {0.3f}'.format(test_acc))\n",
    "  def visualize(self):\n",
    "    pass\n",
    "    # GRAPH EXAMPLES https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "#     pyplot.plot(self.history.history['loss'], label='train')\n",
    "#     pyplot.plot(self.history.history['val_loss'], label='test')\n",
    "#     pyplot.legend()\n",
    "#     pyplot.show()\n",
    "\n",
    "\n",
    "# ## LSTM & GRU Model\n",
    "\n",
    "# In[234]:\n",
    "\n",
    "\n",
    "class RNNModel(MLModel):\n",
    "    def __init__(\n",
    "      # GRID SEARCH & KERAS CLASSIFIER EXAMPLE https://www.kaggle.com/shujunge/gridsearchcv-with-keras\n",
    "      self, \n",
    "      input_shape,\n",
    "      output_dim,\n",
    "      param_grid,\n",
    "      scoring=None, refit=True, verbose=2,\n",
    "      output_file=\"RNN_best_model.h5\",\n",
    "      early_stopping_monitor='val_loss',\n",
    "      model_checkpoint_monitor='val_loss',\n",
    "    ):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_dim  = output_dim\n",
    "        self.output_file = output_file\n",
    "        self.callbacks   =  [ \n",
    "           EarlyStopping(monitor=early_stopping_monitor, mode='min', verbose=verbose) , \n",
    "           ModelCheckpoint( \n",
    "               self.output_file , monitor=model_checkpoint_monitor, \n",
    "               mode='auto', save_best_only=True, verbose=verbose\n",
    "           ),\n",
    "           CSVLogger('train_callback_log.txt', append=True),\n",
    "           TensorBoard(\n",
    "                log_dir='logs', histogram_freq=0, write_graph=True, write_images=False,\n",
    "                update_freq='epoch', profile_batch=2, embeddings_freq=0,\n",
    "                embeddings_metadata=None\n",
    "            ), \n",
    "            ProgbarLogger()\n",
    "        ]\n",
    "        print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"INITIALIZING GRID SEARCH RNN MODEL\" , 10*\"_ \"))\n",
    "        # CALLBACKS EXPLANATION  https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "        self.model = GridSearchCV(\n",
    "            estimator  = KerasClassifier( build_fn = self.create_model,  verbose=verbose),\n",
    "            param_grid = param_grid,\n",
    "            # scoring    = 'accuracy' , #['accuracy', 'precision'], \n",
    "            # refit      = 'precision',                  # For multiple metric evaluation, this needs to be a string denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.\n",
    "            n_jobs     = 1,#-1,                           # -1 means using all processors.\n",
    "            pre_dispatch = \"1*n_jobs\",\n",
    "            cv         = 10, \n",
    "            return_train_score = True,\n",
    "            scoring=scoring, \n",
    "            refit=refit, \n",
    "            verbose=verbose,\n",
    "        )\n",
    "        print(\"\"\"\n",
    "        PARAMETERS:\n",
    "        ________________________________\n",
    "        input_shape :  {}\n",
    "        output_dim  :  {}\n",
    "        main scoring:  {}\n",
    "        all scoring :  {}\n",
    "        early_stopping_monitor   : {}\n",
    "        model_checkpoint_monitor : {}\n",
    "        verbose: {}\n",
    "        \"\"\".format( input_shape, output_dim , refit , scoring, early_stopping_monitor, model_checkpoint_monitor, verbose))\n",
    "        for k in param_grid: print( \"{} : {}\".format(k, param_grid[k] ) )\n",
    "        print(\"\\n\\n\")\n",
    "    def create_model(\n",
    "        self, hidden_layers, hidden_layers_neurons, loss, optimizer, \n",
    "        rnn_hidden_layers, rnn_hidden_layers_neurons, modelType=\"LSTM\", \n",
    "        dropout=True, dropout_rate=0.2, output_layer_activation=\"relu\", \n",
    "        rnn_layer_activation=\"relu\", hidden_layer_activation=\"relu\",  \n",
    "    ):\n",
    "        keras_eval_metric = [\n",
    "            [\n",
    "                tf.keras.metrics.TruePositives(name='tp'),\n",
    "                tf.keras.metrics.FalsePositives(name='fp'),\n",
    "                tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "                tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "                tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(name='auc'),\n",
    "            ]\n",
    "        ]\n",
    "        print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"CREATING ML MODEL\" , 10*\"_ \"))\n",
    "        print(\"\"\"\n",
    "        PARAMETERS:\n",
    "        ________________________________ \n",
    "          rnn_hidden_layers:         {} \n",
    "          rnn_hidden_layers_neurons: {} \n",
    "          hidden_layers:             {} \n",
    "          hidden_layers_neurons:     {}\n",
    "          loss:                      {}\n",
    "          optimizer:                 {}\n",
    "          modelType:                 {}\n",
    "          dropout:                   {}\n",
    "          dropout_rate:              {}\n",
    "          input_shape:               {}\n",
    "          output_dim:                {}\n",
    "          output_layer_activation:   {}\n",
    "          rnn_layer_activation:      {}\n",
    "          hidden_layer_activation:   {}\n",
    "          keras_eval_metric:         {}\n",
    "          callbacks:                 {}\n",
    "          \\n\"\"\".format(\n",
    "            rnn_hidden_layers, rnn_hidden_layers_neurons, hidden_layers, hidden_layers_neurons, loss, optimizer, \n",
    "            modelType, dropout, dropout_rate, self.input_shape, self.output_dim, output_layer_activation,\n",
    "            rnn_layer_activation, hidden_layer_activation, \n",
    "            keras_eval_metric, self.callbacks\n",
    "        ))\n",
    "        model = Sequential()\n",
    "        #INPUT DIM EXPLANATION https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "        \n",
    "        if(modelType == \"LSTM\"):\n",
    "            model.add(LSTM(units=rnn_hidden_layers_neurons , input_shape=self.input_shape, activation=rnn_layer_activation  ))\n",
    "        elif(modelType == \"GRU\"):\n",
    "            model.add(GRU( units=rnn_hidden_layers_neurons , input_shape=self.input_shape, activation=rnn_layer_activation  ))\n",
    "        \n",
    "        for i in range(rnn_hidden_layers):\n",
    "            if(modelType == \"LSTM\"):\n",
    "                model.add(LSTM(units=rnn_hidden_layers_neurons , activation=rnn_layer_activation  ))\n",
    "            elif(modelType == \"GRU\"):\n",
    "                model.add(GRU( units=rnn_hidden_layers_neurons , activation=rnn_layer_activation  ))\n",
    "        \n",
    "        for i in range(hidden_layers):\n",
    "            model.add(Dense(hidden_layers_neurons))\n",
    "            model.add(Activation(hidden_layer_activation))\n",
    "        if(dropout):\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(self.output_dim )) # model.add(Dense(1, activation='sigmoid' )) 'softmax' \n",
    "        model.add(Activation(output_layer_activation))\n",
    "        model.compile(loss=loss, optimizer=optimizer, \n",
    "                      metrics=keras_eval_metric \n",
    "                     ) #, metrics=['accuracy']\n",
    "        print(\"MODEL SUMMARY: \\n\\n\", model.summary())\n",
    "        self.modelType = modelType\n",
    "        self.model = model\n",
    "        return model\n",
    "    def train(self, X, y, X_test, y_test):\n",
    "        print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"TRAINING RNN\" , 10*\"_ \"))\n",
    "        start=time.time()\n",
    "        # ClASS WEIGHTS COMPUTATION https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
    "        class_weights = class_weight.compute_class_weight('balanced', np.unique(y.flatten()), y.flatten())\n",
    "        print(\"\"\"\n",
    "        Class weights: \\n{}\\n{}\\n\n",
    "        for classes: \\n{}\\n\n",
    "        # Frauds: {}\n",
    "        # of Non-Frauds: {}\n",
    "        \"\"\".format( \n",
    "            class_weights, \n",
    "            dict(enumerate(class_weights)),\n",
    "            np.unique(y.flatten()), \n",
    "            np.count_nonzero(y == 1), \n",
    "            np.count_nonzero(y == 0),\n",
    "        ))\n",
    "        print(\"\"\"INPUTS\n",
    "        X:      {}\n",
    "        y:      {}\n",
    "        X_test: {}\n",
    "        y_test: {}\n",
    "        \"\"\".format(X.shape, y.shape, X_test.shape, y_test.shape ))\n",
    "        self.history = self.model.fit(\n",
    "          X, y, \n",
    "          validation_data= (X_test, y_test),\n",
    "          class_weight   = class_weights,\n",
    "          callbacks      = self.callbacks   \n",
    "        )\n",
    "        print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"RNN TRAINING RESULTS\" , 10*\"_ \"))\n",
    "        print(\"\"\"\n",
    "          BEST ESTIMATOR:          {} \n",
    "          BEST SCORE:              {}\n",
    "          BEST PARAMS:             {}\n",
    "          BEST INDEX IN CV SEARCH: {}\n",
    "          SCORER FUNCTIONS:        {}\n",
    "          \\n\n",
    "          HISTORY OBJ:             {}        \n",
    "        \\n\\n\"\"\".format( \n",
    "          self.history.best_estimator_,\n",
    "          self.history.best_score_ , \n",
    "          self.history.best_params_ ,\n",
    "          self.history.best_index_,\n",
    "          self.history.scorer_,\n",
    "          self.history\n",
    "        ))\n",
    "        print(\"cv_results_dict: \")\n",
    "        print(pd.DataFrame( self.history.cv_results_ ))\n",
    "        # for params, mean_score, scores in self.history.cv_results_:\n",
    "        # for params, mean_score, scores in self.history.grid_scores_:\n",
    "          # print(\"\\tMean: {}. Std: {}. Params: {}\".format(scores.mean(), scores.std(), params))\n",
    "        print(\"Total time: {:0.2f}  seconds or {:0.2f} minutes. Saving model to: {}\".format(  time.time()-start , (time.time()-start)/60, self.output_file ))\n",
    "        return self.history\n",
    "\n",
    "\n",
    "# # Run\n",
    "\n",
    "# ## Generate Data\n",
    "\n",
    "# In[164]:\n",
    "\n",
    "\n",
    "data, X_train, X_test, y_train, y_test, labels_hash = None, None, None, None, None, None\n",
    "print(\"\"\"\n",
    "    DATA PREPROCESSING\n",
    "    DOWNLOAD FROM KAGGLE: {}\n",
    "    GENERATE DATA:        {}\n",
    "    READ FROM CLOUD:      {}\n",
    "    SAVE TO CLOUD:        {}\n",
    "\"\"\".format(download_data, generate_data, read_from_cloud, save_to_cloud))\n",
    "\n",
    "\n",
    "# In[165]:\n",
    "\n",
    "\n",
    "if(download_data):\n",
    "    downloadFromKaggle(\n",
    "        api_token = {\"username\":\"rubencg195\",\"key\":\"1a0667935c03c900bf8cc3b4538fa671\"},\n",
    "        kaggle_file_path='/home/ec2-user/.kaggle/kaggle.json',\n",
    "        zip_file_path = \"banksim1.zip\"\n",
    "    )\n",
    "    \n",
    "data = read_data(input_file_path=\"bs140513_032310.csv\")\n",
    "\n",
    "\n",
    "# In[166]:\n",
    "\n",
    "\n",
    "if(generate_data):\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \" GENERATE DATA \" , 10*\"_ \"))\n",
    "    visualize_data(data)\n",
    "\n",
    "\n",
    "# In[167]:\n",
    "\n",
    "\n",
    "if(generate_data):\n",
    "    labels_hash                        = normalizing_data(data)\n",
    "\n",
    "\n",
    "# In[168]:\n",
    "\n",
    "\n",
    "if(generate_data):\n",
    "    rnn_data, smaller_batches_rnn_data = generating3DRNNInput(data) \n",
    "\n",
    "\n",
    "# In[169]:\n",
    "\n",
    "\n",
    "if(generate_data):\n",
    "    if(reduce_data_for_testing):\n",
    "        print(\"REDUCE DATA FOR TESTING. DATA REDUCED FROM {} TO {}\".format(rnn_data.shape[0], reduce_data_for_testing_value))\n",
    "        rnn_data = rnn_data[:reduce_data_for_testing_value]\n",
    "    rnn_mod_data                       = generateNewFeatures(rnn_data)\n",
    "\n",
    "\n",
    "# In[170]:\n",
    "\n",
    "\n",
    "if(generate_data):\n",
    "    X, grouped_X, y, grouped_y         = separateInBatches(rnn_mod_data, min_batch_size=25)\n",
    "\n",
    "\n",
    "# In[171]:\n",
    "\n",
    "\n",
    "if(generate_data):\n",
    "    #X, y                              = separateLabel(rnn_mod_data)\n",
    "    X_norm                             = normalize3DInput(X)\n",
    "    \n",
    "    #Change all padding values to 0 in the labels\n",
    "    print(\"# of padding values in labels :\", len(y[y==empty_padding_value]))\n",
    "    y[y==empty_padding_value] = 0\n",
    "    #print(\"# Frauds {} vs # Non-Frauds {}. Total: {}\".format(   len(y[y==0]), len(y[y==1]), len(y.reshape(-1))  )) \n",
    "    #print(\"% Frauds {:0.2f}% vs % Non-Frauds {:0.2f}%\".format(  ( len(y[y==1]) / len(y.reshape(-1)) ) * 100,  ( len(y[y==0]) / len(y.reshape(-1)) ) * 100 ))\n",
    "    X_train, X_test, y_train, y_test, X_val, y_val = separatingTrainTest(X_norm, y)\n",
    "\n",
    "    pickle.dump( rnn_data     , open( \"rnn_data.data\"      , \"wb\" ) ) \n",
    "    pickle.dump( rnn_mod_data , open( \"rnn_mod_data.data\"  , \"wb\" ) ) \n",
    "    pickle.dump( X_train      , open( \"X_train.data\"       , \"wb\" ) ) \n",
    "    pickle.dump( X_test       , open( \"X_test.data\"        , \"wb\" ) )\n",
    "    pickle.dump( X_val        , open( \"X_val.data\"         , \"wb\" ) )\n",
    "    pickle.dump( y_train      , open( \"y_train.data\"       , \"wb\" ) )\n",
    "    pickle.dump( y_test       , open( \"y_test.data\"        , \"wb\" ) )\n",
    "    pickle.dump( y_val        , open( \"y_val.data\"         , \"wb\" ) )\n",
    "    pickle.dump( labels_hash  , open( \"labels_hash.data\"   , \"wb\" ) )\n",
    "    \n",
    "    runCommands([\"ls *.png\", \"ls *.data\", \"ls *.h5\"])\n",
    "    print(\"\"\"SHAPES & KEYS:\n",
    "    X_train          : {}\n",
    "    y_train          : {}\n",
    "    ________________________\n",
    "    X_test           : {}\n",
    "    y_test           : {}\n",
    "    ________________________\n",
    "    X_val            : {}\n",
    "    y_val            : {}\n",
    "    ________________________\n",
    "    labels_hash Keys : {}\n",
    "    \"\"\".format(\n",
    "        X_train.shape, y_train.shape,\n",
    "        X_test.shape,  y_test.shape,\n",
    "        X_val.shape, y_val.shape, \n",
    "        labels_hash.keys() \n",
    "    ))\n",
    "    \n",
    "    columns=[\n",
    "        \"day\", \"age\", \"gender\", \"merchant\", \"category\", \"amount\", \n",
    "        \"curr_day_tr_n\",\"ave_tr_p_day_amount\", \"tot_ave_tr_amount\", \"is_mer_new\",\"com_tr_type\", \"com_mer\",\n",
    "        \"fraud\"\n",
    "    ]\n",
    "    excel_filename = 'data.xlsx'\n",
    "    print(\"SAVING DATA TO EXCEL: \", excel_filename)\n",
    "    writer = pd.ExcelWriter(excel_filename, engine='xlsxwriter')\n",
    "    bar    = progressbar.ProgressBar(max_value=len(rnn_mod_data))\n",
    "    for i,b in enumerate(rnn_mod_data):\n",
    "        pd.DataFrame(b, columns=columns).to_excel(writer, sheet_name='cust_{}'.format(i))\n",
    "        bar.update(i+1)\n",
    "    writer.save()\n",
    "\n",
    "#     excel_filename = 'rnn_data.xlsx'\n",
    "#     print(\"SAVING 3D NORM DATA TO EXCEL: \", excel_filename)\n",
    "#     writer = pd.ExcelWriter(excel_filename, engine='xlsxwriter')\n",
    "#     bar    = progressbar.ProgressBar(max_value=len(X))\n",
    "#     for i,b in enumerate(X):\n",
    "#         pd.DataFrame(b, columns=columns[:-1]).to_excel(writer, sheet_name='i_{}_lbl_{}'.format(i, y[i]))\n",
    "#         bar.update(i+1)\n",
    "#     writer.save()\n",
    "\n",
    "else:\n",
    "    if(read_from_cloud):\n",
    "        X_train, X_test, y_train, y_test, labels_hash = readDataFromCloud()\n",
    "    else:\n",
    "        X_train, y_train,  X_test, y_test, X_val, y_val, labels_hash  = readLocally()\n",
    "        \n",
    "    if(reduce_data_for_testing):\n",
    "        print(\"REDUCE TRAIN DATA FOR TESTING. DATA REDUCED FROM {} TO {}\".format(X_train.shape[0], reduce_data_for_testing_value))\n",
    "        X_train = X_train[:reduce_data_for_testing_value]\n",
    "        X_test  = X_test[:reduce_data_for_testing_value]\n",
    "        X_val   = X_val[:reduce_data_for_testing_value]\n",
    "        y_train = y_train[:reduce_data_for_testing_value]\n",
    "        y_test  = y_test[:reduce_data_for_testing_value]\n",
    "        y_val   = y_val[:reduce_data_for_testing_value]\n",
    "\n",
    "\n",
    "# ## Model Setup\n",
    "\n",
    "# In[235]:\n",
    "\n",
    "\n",
    "#ONLY FOR TF 1.15\n",
    "# session = tf.keras.backend.get_session()\n",
    "# init = tf.global_variables_initializer()\n",
    "# session.run(init)\n",
    "\n",
    "param_grid = {\n",
    "    \"rnn_hidden_layers\"         : [0, 1], \n",
    "    \"rnn_hidden_layers_neurons\" : [50, 100],  #[25, 50, 100], \n",
    "    \"hidden_layers\"             : [2],  #[0, 1, 2], \n",
    "    \"hidden_layers_neurons\"     : [200, 300],  #[25, 50, 100], \n",
    "    \"loss\"                      : ['binary_crossentropy'], # [\"mse\"], \n",
    "    \"optimizer\"                 : ['adam'], #[tf.keras.optimizers.SGD(lr=0.01)], #['adam'],\n",
    "    \"modelType\"                 : ['LSTM', 'GRU'],\n",
    "    \"epochs\"                    : [50], #[25, 50], # [1],\n",
    "    \"output_layer_activation\"   : ['sigmoid'], #['relu'] #['softmax']  # ['sigmoid']\n",
    "    \"rnn_layer_activation\"      : [\"sigmoid\"], \n",
    "    \"hidden_layer_activation\"   : [\"sigmoid\"],\n",
    "    \"dropout\"                   : [True],\n",
    "    \"dropout_rate\"              : [0.2]\n",
    "}\n",
    "if(reduce_data_for_testing):\n",
    "    param_grid = {\n",
    "        \"rnn_hidden_layers\"         : [0, 1], \n",
    "        \"rnn_hidden_layers_neurons\" : [50], \n",
    "        \"hidden_layers\"             : [2],  \n",
    "        \"hidden_layers_neurons\"     : [50],\n",
    "        \"loss\"                      : ['binary_crossentropy'], \n",
    "        \"optimizer\"                 : ['adam'],\n",
    "        \"modelType\"                 : ['LSTM'],\n",
    "        \"epochs\"                    : [1],\n",
    "        \"output_layer_activation\"   : ['sigmoid'],\n",
    "        \"rnn_layer_activation\"      : [\"sigmoid\"], \n",
    "        \"hidden_layer_activation\"   : [\"sigmoid\"],\n",
    "        \"dropout\"                   : [True],\n",
    "        \"dropout_rate\"              : [0.2]\n",
    "    }\n",
    "\n",
    "n_batches        = X_train.shape[0]\n",
    "batch_size       = X_train.shape[1]\n",
    "n_features       = X_train.shape[2]\n",
    "# n_pred_per_batch = y_train.shape[1]\n",
    "rnn = RNNModel(\n",
    "  input_shape=( batch_size , n_features  ),\n",
    "  output_dim = 1,\n",
    "  param_grid=param_grid,\n",
    "  scoring=['accuracy', 'precision', 'recall', 'roc_auc', 'f1', 'average_precision' ],             # scoring=None, \n",
    "  refit= \"recall\", #'accuracy',        #\"precision\" , \"recall\"  100 recall catch everything                                                                   #True, \n",
    "#   scoring=None, \n",
    "#   refit=True, \n",
    "  verbose=2,\n",
    "  output_file= model_name,\n",
    "  early_stopping_monitor=\"val_recall\",#'val_loss',\n",
    "  model_checkpoint_monitor=\"val_recall\", #'val_accuracy',\n",
    ")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "history = rnn.train( X_train, y_train, X_test, y_test )\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "print(\"Saving best estimator at {} and weights at {}\".format(\"rnn_model.h5\", \"rnn_model_weights.h5\"))\n",
    "import joblib\n",
    "import copy\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print(rnn.model.best_estimator_.model)\n",
    "rnn.model.best_estimator_.model.save_weights(\"rnn_model_weights.h5\")\n",
    "rnn.model.best_estimator_.model.save(\"rnn_model.h5\")\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# runCommand(\"ls *.h5\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# print(pd.DataFrame(history.cv_results_))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(history.__dict__)\n",
    "\n",
    "\n",
    "# ## Visualization\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "cv_results_df = pd.DataFrame(history.cv_results_).round(3)\n",
    "\n",
    "try:\n",
    "    cv_results_df.to_csv(\"cv_results.history\")\n",
    "    cv_results_df.to_csv(\"cv_results.csv\")\n",
    "except:\n",
    "    print(\"Couldn't save cv_results.history\")\n",
    "\n",
    "for i, cvr in enumerate(cv_results_df.iterrows()):\n",
    "    print(\"\"\"{} MODEL # {} {}\\n\n",
    "        PARAM EPOCHS    :  {} HIDDEN LAYERS  :  {}   NEURONS PER HIDDEN LAYER:  {} \n",
    "        LOSS FUNCTION   : \"{}\" MODEL TYPE     : \"{}\" OPTIMIZER        : \"{}\"\n",
    "        STD FIT TIME    :  {} MEAN SCORE TIME:  {} STD SCORE TIME   :   {} \n",
    "        MEAN TEST SCORE :  {} STD TEST SCORE :  {} RANK TEST SCORE  :   {} \n",
    "        MEAN TRAIN SCORE:  {} STD TRAIN SCORE:  {} \n",
    "        PARAMS: {}      \n",
    "    \"\"\".format(\n",
    "        10*\" _\" , i+1, 10*\" _\", \n",
    "        cv_results_df.at[i, \"param_epochs\"]     , cv_results_df.at[i, \"param_hidden_layers\"]  , cv_results_df.at[i, \"param_hidden_layers_neurons\"], cv_results_df.at[i, \"param_rnn_hidden_layers\"] ,  cv_results_df.at[i, \"param_rnn_hidden_layers_neurons\"] , \n",
    "        cv_results_df.at[i, \"param_loss\"]       , cv_results_df.at[i, \"param_modelType\"]      , cv_results_df.at[i, \"param_optimizer\"]            ,\n",
    "        cv_results_df.at[i, \"std_fit_time\"]     , cv_results_df.at[i, \"mean_score_time\"]      , cv_results_df.at[i, \"std_score_time\"]             ,                                                                                                                      \n",
    "#         cv_results_df.at[i, \"mean_test_score\"]  , cv_results_df.at[i, \"std_test_score\"]       , cv_results_df.at[i, \"rank_test_score\"]            ,\n",
    "        cv_results_df.at[i, \"mean_test_accuracy\"]  , cv_results_df.at[i, \"std_test_accuracy\"]       , cv_results_df.at[i, \"rank_test_accuracy\"]            ,\n",
    "        cv_results_df.at[i, \"mean_train_accuracy\"] , cv_results_df.at[i, \"std_train_accuracy\"]      ,\n",
    "#         cv_results_df.at[i, \"mean_train_score\"] , cv_results_df.at[i, \"std_train_score\"]      ,\n",
    "        cv_results_df.at[i, \"params\"]                     \n",
    "    ))\n",
    "#     train_score_p_split = [ cv_results_df.at[i,\"split{}_train_score\".format(j)] for j in range(10) ]\n",
    "#     test_score_p_split  = [ cv_results_df.at[i,\"split{}_test_score\".format(j)] for j in range(10) ]\n",
    "    train_score_p_split = [ cv_results_df.at[i,\"split{}_train_accuracy\".format(j)] for j in range(10) ]\n",
    "    test_score_p_split  = [ cv_results_df.at[i,\"split{}_test_accuracy\".format(j)] for j in range(10) ]\n",
    "    \n",
    "    train_pre_p_split = [ cv_results_df.at[i,\"split{}_train_precision\".format(j)] for j in range(10) ]\n",
    "    test_pre_p_split  = [ cv_results_df.at[i,\"split{}_test_precision\".format(j)] for j in range(10) ]\n",
    "    \n",
    "    train_rec_p_split = [ cv_results_df.at[i,\"split{}_train_recall\".format(j)] for j in range(10) ]\n",
    "    test_rec_p_split  = [ cv_results_df.at[i,\"split{}_test_recall\".format(j)] for j in range(10) ]\n",
    "    \n",
    "    train_roc_auc_p_split = [ cv_results_df.at[i,\"split{}_train_roc_auc\".format(j)] for j in range(10) ]\n",
    "    test_roc_auc_p_split  = [ cv_results_df.at[i,\"split{}_test_roc_auc\".format(j)] for j in range(10) ]\n",
    "    \n",
    "    train_average_precision_p_split = [ cv_results_df.at[i,\"split{}_train_average_precision\".format(j)] for j in range(10) ]\n",
    "    test_average_precision_p_split  = [ cv_results_df.at[i,\"split{}_test_average_precision\".format(j)] for j in range(10) ]\n",
    "    \n",
    "    train_f1_p_split = [ cv_results_df.at[i,\"split{}_train_f1\".format(j)] for j in range(10) ]\n",
    "    test_f1_p_split  = [ cv_results_df.at[i,\"split{}_test_f1\".format(j)] for j in range(10) ]\n",
    "    \n",
    "    index_titles = [\"TRAIN\", \"TEST\"]\n",
    "\n",
    "    print(\"\"\"\\nACC PERFORMANCE PER SPLIT \\n\\n{}\n",
    "        \\n\\n\"\"\".format( pd.DataFrame([ np.round(train_score_p_split, 3) , np.round(test_score_p_split, 3) ], \n",
    "            columns=[ \"SPLIT#{}\".format(se ) for se in range(10)],\n",
    "            index=index_titles\n",
    "    )))\n",
    "    \n",
    "    print(\"\"\"\\nPREC PERFORMANCE PER SPLIT \\n\\n{}\n",
    "        \\n\\n\"\"\".format( pd.DataFrame([ np.round(train_pre_p_split, 3) , np.round(test_pre_p_split, 3) ], \n",
    "            columns=[ \"SPLIT#{}\".format(se ) for se in range(10)],\n",
    "            index=index_titles\n",
    "    )))\n",
    "    \n",
    "    print(\"\"\"\\nREC PERFORMANCE PER SPLIT \\n\\n{}\n",
    "        \\n\\n\"\"\".format( pd.DataFrame([ np.round(train_rec_p_split, 3) , np.round(test_rec_p_split, 3) ], \n",
    "            columns=[ \"SPLIT#{}\".format(se ) for se in range(10)],\n",
    "            index=index_titles\n",
    "    )))\n",
    "    \n",
    "    print(\"\"\"\\nROC AUC PERFORMANCE PER SPLIT \\n\\n{}\n",
    "        \\n\\n\"\"\".format( pd.DataFrame([ np.round(train_roc_auc_p_split, 3) , np.round(test_roc_auc_p_split, 3) ], \n",
    "            columns=[ \"SPLIT#{}\".format(se ) for se in range(10)],\n",
    "            index=index_titles\n",
    "    )))\n",
    "    \n",
    "    print(\"\"\"\\nAVE PRE PERFORMANCE PER SPLIT \\n\\n{}\n",
    "        \\n\\n\"\"\".format( pd.DataFrame([ np.round(train_average_precision_p_split, 3) , np.round(test_average_precision_p_split, 3) ], \n",
    "            columns=[ \"SPLIT#{}\".format(se ) for se in range(10)],\n",
    "            index=index_titles\n",
    "    )))\n",
    "        \n",
    "    print(\"\"\"\\nF1 PERFORMANCE PER SPLIT \\n\\n{}\n",
    "        \\n\\n\"\"\".format( pd.DataFrame([ np.round(train_f1_p_split, 3) , np.round(test_f1_p_split, 3) ], \n",
    "            columns=[ \"SPLIT#{}\".format(se ) for se in range(10)],\n",
    "            index=index_titles\n",
    "    )))\n",
    "    \n",
    "    \n",
    "#     plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "#     plt.figure(figsize=(100,40))\n",
    "#     plt.rcParams.update({'font.size': 130})\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_score_p_split)\n",
    "    plt.plot(test_score_p_split)\n",
    "    plt.title('model accuracy')\n",
    "    plt.text(1, 1, str(cv_results_df.at[i, \"params\"]), fontsize=8, style='oblique', ha='center',\n",
    "         va='top', wrap=True)\n",
    "    print( str(cv_results_df.at[i, \"params\"])  )\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('split')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.draw()\n",
    "    img_name = \"Model#{}_acc.png\".format(i+1)\n",
    "    print(\"\\n\\nSaving image with name: \",  img_name )\n",
    "    plt.savefig( img_name)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_rec_p_split)\n",
    "    plt.plot(train_rec_p_split)\n",
    "    plt.title('model precision')\n",
    "    plt.ylabel('precicion')\n",
    "    plt.xlabel('split')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.draw()\n",
    "    img_name = \"Model#{}_prec.png\".format(i+1)\n",
    "    print(\"\\n\\nSaving image with name: \",  img_name )\n",
    "    plt.savefig( img_name)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_rec_p_split)\n",
    "    plt.plot(test_rec_p_split)\n",
    "    plt.title('model recall')\n",
    "    plt.ylabel('recall')\n",
    "    plt.xlabel('split')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.draw()\n",
    "    img_name = \"Model#{}_rec.png\".format(i+1)\n",
    "    print(\"\\n\\nSaving image with name: \",  img_name )\n",
    "    plt.savefig( img_name)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_roc_auc_p_split)\n",
    "    plt.plot(test_roc_auc_p_split)\n",
    "    plt.title('model auc')\n",
    "    plt.ylabel('auc')\n",
    "    plt.xlabel('split')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.draw()\n",
    "    img_name = \"Model#{}_auc.png\".format(i+1)\n",
    "    print(\"\\n\\nSaving image with name: \",  img_name )\n",
    "    plt.savefig( img_name)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "selected_epochs = history.cv_results_[\"params\"][0]['epochs']\n",
    "print(\"\"\"\\n\\nBEST MODEL HISTORY PER EPOCH\n",
    "SELECTED EPOCHS   : {}\n",
    "PARAMS            : {} \\n\"\"\".format( \n",
    "  selected_epochs,\n",
    "  history.best_params_\n",
    "))\n",
    "\n",
    "index_names = [\n",
    "    \"accuracy\", \"val_accuracy\",\n",
    "    \"loss\", \"val_loss\", \n",
    "    \"precision\", \"val_precision\", \n",
    "    \"recall\", \"val_recall\", \n",
    "    #\"roc_auc\", \"val_roc_auc\", \n",
    "    #\"f1\", \"val_f1\", \n",
    "#     \"average_precision\", \"average_precision\"\n",
    "]\n",
    "index_titles = [\n",
    "    \"TRAIN ACC\", \"TEST ACC\", \n",
    "    \"TRAIN LOSS\", \"TEST LOSS\",\n",
    "    \"precision\", \"val_precision\", \n",
    "    \"recall\", \"val_recall\", \n",
    "    #\"roc_auc\", \"val_roc_auc\", \n",
    "    #\"f1\", \"val_f1\", \n",
    "#     \"average_precision\", \"average_precision\"\n",
    "]\n",
    "print( pd.DataFrame([ np.round( history.best_estimator_.model.history.history[iname] , 3) for iname in index_names], \n",
    "#   columns=[ \"EPOCH#{}\".format(se+1) for se in range(selected_epochs)],\n",
    "  index=index_titles\n",
    "), \"\\n\")\n",
    "\n",
    "# summarize history for accuracy\n",
    "fig = plt.figure()\n",
    "plt.plot(history.best_estimator_.model.history.history['accuracy'])\n",
    "plt.plot(history.best_estimator_.model.history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.draw()\n",
    "plt.savefig( 'BEST MODEL ACC' )\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(history.best_estimator_.model.history.history['loss'])\n",
    "plt.plot(history.best_estimator_.model.history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.draw()\n",
    "plt.savefig( 'BEST MODEL LOSS' )\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(rnn.model.best_estimator_)\n",
    "y_pred = rnn.model.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "cm_data = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "cm_df   = pd.DataFrame(cm_data, index=[0,1], columns=[0,1]).rename_axis('True').rename_axis('Predicted', axis=1)\n",
    "# ax.set_xlabel(\"x label\")\n",
    "# cm_df.set_ylabel(\"y label\")\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(cm_df)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(\"tn, fp, fn, tp\")\n",
    "print(tn, fp, fn, tp)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def plot_roc_auc(y_test, preds):\n",
    "    '''\n",
    "    Takes actual and predicted(probabilities) as input and plots the Receiver\n",
    "    Operating Characteristic (ROC) curve\n",
    "    ''' \n",
    "    fig = plt.figure()\n",
    "    fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"fpr: \", fpr)\n",
    "    print(\"tpr: \", tpr)\n",
    "    print(\"ROC AUC: \", roc_auc)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.draw()\n",
    "    plt.savefig( 'ROCAUC.png' )\n",
    "   \n",
    "plot_roc_auc(y_test, y_pred)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def pr_curve(y_test, y_pred):\n",
    "    fig = plt.figure()\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "    average_precision = average_precision_score(y_test, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    plt.title('PR CURVE')\n",
    "    print(\"PREC: \", precision)\n",
    "    print(\"REC: \", recall)\n",
    "    print(\"pr_auc: \", pr_auc)\n",
    "    print(\"average_precision: \", average_precision)\n",
    "    plt.plot(recall, precision, 'b', label = 'Ave PRE = %0.2f' % average_precision)\n",
    "    plt.legend(loc = 'lower right')\n",
    "#     plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.draw()\n",
    "    plt.savefig( 'PRCURVE.png' )\n",
    "    print(\"P: {}\\nR: {}\\nTHRES: {}\".format(precision, recall, thresholds))\n",
    "    \n",
    "pr_curve(y_test, y_pred)\n",
    "\n",
    "\n",
    "# ## Save Data\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# root_path = \"/home/ec2-user/SageMaker\"\n",
    "# _ = runCommand(\"ls *.png\")\n",
    "# _ = runCommand(\"ls *.data\")\n",
    "# _ = runCommand(\"ls *.h5\")\n",
    "# _ = runCommand(\"ls *.model\")\n",
    "# _ = runCommand(\"ls *.png\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# if(save_to_cloud):\n",
    "#     saveToCloud( X_train, X_test, y_train, y_test, X_val, y_val, history, rnn, \"rnn.model\" )\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import traceback\n",
    "\n",
    "def testTry(fn, addErrMsg=\"\"):\n",
    "    try:\n",
    "        fn()\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", addErrMsg,\"-\", repr(e))\n",
    "#         traceback.print_exc()\n",
    "        \n",
    "import copy\n",
    "historyDict = copy.copy(history.__dict__)\n",
    "print(historyDict.keys())\n",
    "historyDict.pop(\"best_estimator_\")\n",
    "# historyDict.pop(\"multimetric_\")\n",
    "# historyDict.pop(\"scorer_\")\n",
    "historyDict.pop(\"estimator\")   \n",
    "# testTry(lambda : historyDict[\"best_params_\"].pop('keras_eval_metric') , \"best_params_\" )\n",
    "# testTry(lambda : historyDict[\"cv_results_\"].pop('param_dropout'), \"cv_results_\" )\n",
    "# testTry(lambda : historyDict[\"cv_results_\"].pop('param_dropout_rate'), \"cv_results_\" )\n",
    "# testTry(lambda : historyDict[\"cv_results_\"].pop('param_keras_eval_metric'), \"cv_results_\" )\n",
    "# testTry(lambda : historyDict[\"param_grid\"].pop('keras_eval_metric'), \"param_grid\" )\n",
    "\n",
    "\n",
    "print(\"Saving history\")\n",
    "joblib.dump( historyDict, \"rnn.history\")\n",
    "print(\"History saved successfully.\")\n",
    "\n",
    "print(\"Saving history per parts\")\n",
    "for k in history.__dict__.keys():\n",
    "    try:\n",
    "        print(\"Saving \", k, \"->\", \"{}_rnn.history\".format(k), \"\\n\",  \n",
    "        history.__dict__[k].keys() if type(history.__dict__[k]) == dict else history.__dict__[k])\n",
    "        joblib.dump(  history.__dict__[k], \"{}_rnn.history\".format(k) )\n",
    "    except Exception as e:\n",
    "        print(\"Error saving:\", \"{}.history\".format(k) ,\"-\", repr(e))\n",
    "        try:\n",
    "            print(\"Saving:\", \"{}.history.txt\".format(k)) \n",
    "            text_file = open(\"{}.history.txt\".format(k), \"w\")\n",
    "            text_file.write(str(history.__dict__[k]))\n",
    "            text_file.close()\n",
    "        except Exception as ee:\n",
    "            print(\"Error saving:\", \"{}.txt\".format(k),\"-\", repr(ee))\n",
    "\n",
    "print(\"History parts saved successfully.\")\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "print(\"BEST ESTIMATOR\")\n",
    "print(rnn.model.best_estimator_.model)\n",
    "print(rnn.model.best_estimator_.model.summary())\n",
    "print(rnn.model.best_estimator_.model.get_config())\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "loaded_rnn = load_model(\"rnn_model.h5\")\n",
    "print(loaded_rnn.summary())\n",
    "print(loaded_rnn.predict(X_test))\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Verafin MITACS 2020",
   "language": "python",
   "name": "verafin-mitacs-2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

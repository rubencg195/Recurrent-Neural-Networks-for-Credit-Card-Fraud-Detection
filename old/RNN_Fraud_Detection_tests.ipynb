{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "kO_S_YizvNSd"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "uGHY3IgdvMpv"
   },
   "outputs": [],
   "source": [
    "generate_data = False\n",
    "save_to_cloud = True\n",
    "bucket_address = \"s3://verafin-mitacs-ruben-chevez/\"\n",
    "project_folder = \"customer_batches\"\n",
    "model_name     = \"customer_batches_rnn_best_model.h5\"\n",
    "project_path   = bucket_address + project_folder \n",
    "\n",
    "#General\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import progressbar\n",
    "import pickle\n",
    "import s3fs\n",
    "\n",
    "#Math & Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "## Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold , StratifiedKFold\n",
    "\n",
    "## Models\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping,  ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['verafin-mitacs-ruben-chevez/customer_batches/RESULTS V1 - ML FAILED CPU .ipynb',\n",
       " 'verafin-mitacs-ruben-chevez/customer_batches/data',\n",
       " 'verafin-mitacs-ruben-chevez/customer_batches/images',\n",
       " 'verafin-mitacs-ruben-chevez/customer_batches/models']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "# To List 5 files in your accessible bucket\n",
    "s3fs.S3FileSystem().ls(project_path)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "oKy1n6zEvGRP"
   },
   "source": [
    "# Import Dataset From Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "hidden": true,
    "id": "AwDk3AMduzUL",
    "outputId": "0554f812-a087-4c9a-eb23-86572be25dc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: kaggle: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf /root/.kaggle\n",
    "# !mkdir /root/.kaggle\n",
    "\n",
    "!rm -rf /home/ec2-user/.kaggle\n",
    "!mkdir /home/ec2-user/.kaggle\n",
    "api_token = {\"username\":\"rubencg195\",\"key\":\"1a0667935c03c900bf8cc3b4538fa671\"}\n",
    "# with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "with open('/home/ec2-user/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)\n",
    "\n",
    "# !chmod 600 /root/.kaggle/kaggle.json\n",
    "!chmod 600 /home/ec2-user/.kaggle/kaggle.json\n",
    "\n",
    "!kaggle datasets download -d ntnu-testimon/banksim1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "XDYr9r9QwgLa"
   },
   "outputs": [],
   "source": [
    "# zipFilePath = \"/content/banksim1.zip\" \n",
    "zipFilePath = \"/home/ec2-user/SageMaker/banksim1.zip\"\n",
    "zip_ref = zipfile.ZipFile(zipFilePath, 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "hidden": true,
    "id": "aqIzLxZjvOnb",
    "outputId": "2f2dd37f-7e01-4a37-d345-b74f3b5c611a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 94492\r\n",
      "drwxr-xr-x  8 ec2-user ec2-user     4096 Feb 12 13:43 .\r\n",
      "drwx------ 23 ec2-user ec2-user     4096 Feb 12 13:43 ..\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user   520078 Feb 11 19:27 2-RNN_Fraud_Detection_tests.ipynb\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user 13712256 Feb 11 19:45 banksim1.zip\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user     1814 Feb 12 01:39 BOX_PLOT_SHOWING_SIZES_OF_BATCHES_GROUPED_BY_CUSTOMER_ID_generating3DRNNInput.png\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user 48986035 Feb 12 13:43 bs140513_032310.csv\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user 32665953 Feb 12 13:43 bsNET140513_032310.csv\r\n",
      "drwxrwxr-x  2 ec2-user ec2-user     4096 Feb 12 13:38 data\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user     1814 Feb 12 01:39 FEATURE_IMPORTANCE_normalizing_data.png\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user     1814 Feb 12 01:39 FRAUD_PERCENTAGE_AT_DIFFERENT_RANGES_OF_TRANSACTION_AMOUNT_visualize_data.png\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user     1814 Feb 12 01:39 FRAUD_VS_NONFRAUD_HISTOGRAM_visualize_data.png\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user     1814 Feb 12 01:39 FREQUENCY_DISTRIBUTION_OF_TRANSACTION_AMOUNTS_visualize_data.png\r\n",
      "drwxrwxr-x  2 ec2-user ec2-user     4096 Feb 11 18:05 .ipynb_checkpoints\r\n",
      "drwxrwxr-x  2 ec2-user ec2-user     4096 Feb 10 14:45 .kaggle\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user   213630 Feb 12 13:05 kernel-setup.err\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user    92887 Feb 12 13:05 kernel-setup.out\r\n",
      "-rwxr-xr-x  1 ec2-user ec2-user     1734 Feb 12 12:55 kernel-setup.sh\r\n",
      "drwx------  2 root     root        16384 Jan 22 19:11 lost+found\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user     1814 Feb 12 01:39 PIE_CHART_FRAUD_VS_NONFRAUD_visualize_data.png\r\n",
      "drwxrwxrwx  5 ec2-user ec2-user     4096 Feb 10 15:51 rl_cartpole_coach_2020-02-10\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user   477394 Feb 12 13:43 RNN_Fraud_Detection_tests.ipynb\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user     1814 Feb 12 01:39 SCATTER_PLOT_SHOWING_SIZES_OF_BATCHES_GROUPED_BY_CUSTOMER_ID_generating3DRNNInput.png\r\n",
      "drwxr-xr-x  2 ec2-user ec2-user     4096 Jan 22 19:11 .sparkmagic\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "ttDcSRs_xrac"
   },
   "source": [
    "# Data Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "BIYkDVBCiZmv"
   },
   "outputs": [],
   "source": [
    "def read_data(input_file_path):\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"IMPORT DATA FROM CSV\" , 10*\"_ \"))\n",
    "  data = pd.read_csv(input_file_path)\n",
    "  print(\"Deleting the columns 'zipcodeOri','zipMerchant' because all the fields are equal.\\n\\n\")\n",
    "  del data['zipcodeOri']\n",
    "  del data['zipMerchant']\n",
    "  print(\"Data Shape: {} \\n\\nPreview: \\n\\n {} \\n\\n Data Information: \\n\".format( data.shape, data.head() ))\n",
    "  print(\"\\n{}\\nDoes it has null values? {}\".format(data.info(), data.isnull().values.any() ))\n",
    "  return data\n",
    "\n",
    "def visualize_data(data):\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"PIE CHART - FRAUD VS NON-FRAUD\" , 10*\"_ \"))\n",
    "  df_fraud= data[data['fraud']==1]\n",
    "  num_transaction_total, num_transaction_fraud = len(data), len(df_fraud)\n",
    "  num_transaction_total, num_transaction_fraud\n",
    "  print(\"Total Transactions: {} \\nTotal Fraud Transactions: {}\".format(num_transaction_total, num_transaction_fraud) )\n",
    "  percent_fraud = round((num_transaction_fraud / num_transaction_total) * 100, 2)\n",
    "  percent_safe = 100 - percent_fraud\n",
    "  percent_fraud, percent_safe\n",
    "  print(\"% Safe Transactions: {} \\n% Fraud Transactions: {}\\n\\n\".format(percent_safe, percent_fraud) ) # plotting pie chart for percentage comparision: 'fraud' vs 'safe-transaction'\n",
    "  fig1, ax1 = plt.subplots()\n",
    "  plt.title(\"Figure 1. Fraud vs Safe Transaction in Percentage\", fontsize = 20)\n",
    "  labels = ['Fraud', 'Safe Transaction']\n",
    "  sizes = [percent_fraud, percent_safe]\n",
    "  explode = (0, 0.7)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "  patches, texts, autotexts = ax1.pie(sizes,  labels=labels, autopct='%1.1f%%', shadow = True, explode=explode, startangle=130, colors = ['#ff6666', '#2d64bc'])\n",
    "  texts[0].set_fontsize(30)\n",
    "  texts[1].set_fontsize(18)\n",
    "  matplotlib.rcParams['text.color'] = 'black'\n",
    "  matplotlib.rcParams[\"font.size\"] = 30\n",
    "  plt.rcParams[\"figure.figsize\"] = [6, 6]\n",
    "  plt.show()\n",
    "  plt.savefig('PIE_CHART_FRAUD_VS_NONFRAUD_visualize_data.png')\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"COLUMN INFORMATION & PREVIEW\" , 10*\"_ \"))\n",
    "  # Extracting # of unique entires per column and their sample values\n",
    "  num_unique = []\n",
    "  sample_col_values = []\n",
    "  for col in data.columns:\n",
    "      num_unique.append(len(data[col].unique()))  # Counting number of unique values per each column\n",
    "      sample_col_values.append(data[col].unique()[:3])  # taking 3 sample values from each column   \n",
    "  # combining the sample values into a a=single string (commas-seperated)\n",
    "  # ex)  from ['hi', 'hello', 'bye']  to   'hi, hello, bye'\n",
    "  col_combined_entries = []\n",
    "  for col_entries in sample_col_values:\n",
    "      entry_string = \"\"\n",
    "      for entry in col_entries:\n",
    "          entry_string = entry_string + str(entry) + ', '\n",
    "      col_combined_entries.append(entry_string[:-2])\n",
    "  # Generating a list 'param_nature' that distinguishes features and targets\n",
    "  param_nature = []\n",
    "  for col in data.columns:\n",
    "      if col == 'fraud':\n",
    "          param_nature.append('Target')\n",
    "      else:\n",
    "          param_nature.append('Feature')\n",
    "  # Generating Table1. Parameters Overview\n",
    "  df_feature_overview = pd.DataFrame(np.transpose([param_nature, num_unique, col_combined_entries]), index = data.columns, columns = ['Parameter Nature', '# of Unique Entries', 'Sample Entries (First three values)'])\n",
    "  print(\"\\nTotal # of Values: {} \\nShape: {} \\n\\n\".format(len(data), data.shape))\n",
    "  print(df_feature_overview)\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FRAUD VS NON-FRAUD AVE. AMOUNT & PERCENTAGE\" , 10*\"_ \"))\n",
    "  df_fraud     = data.loc[data.fraud == 1] \n",
    "  df_non_fraud = data.loc[data.fraud == 0]\n",
    "  fraud_ave_amount_col          = df_fraud.groupby('category')['amount'].mean().round(2)\n",
    "  non_fraud_ave_amount_col      = df_non_fraud.groupby('category')['amount'].mean().round(2)\n",
    "  percentage_fraud_per_category = data.groupby('category')['fraud'].mean().round(3)*100\n",
    "  amount_percentage_table       = pd.concat(\n",
    "      [ fraud_ave_amount_col , non_fraud_ave_amount_col, percentage_fraud_per_category ],\n",
    "      keys=[\"Fraudulent Ave. Amount\",\"Non-Fraudulent Ave. Amount\",\"Fraud Percent(%)\"],\n",
    "      axis=1, \n",
    "      sort=False\n",
    "  ).sort_values(by=['Non-Fraudulent Ave. Amount'])\n",
    "  print(amount_percentage_table)\n",
    "  num_bins = 15                 # Number of sections where data will be segmented to be shown as a bar in the histogram. For example: The first bin is called \"0~500\"\n",
    "  tran_amount = data['amount']\n",
    "  n, bins, patches = plt.hist(tran_amount, num_bins, density = False, stacked = True, facecolor= '#f26a6a', alpha=0.5)\n",
    "  plt.close()\n",
    "  n_fraud = np.zeros(num_bins)\n",
    "  for i in range(num_bins):\n",
    "      for j in range(num_transaction_fraud):\n",
    "          if bins[i] < df_fraud['amount'].iloc[j] <= bins[i+1]:  #??????\n",
    "              n_fraud[i] += 1\n",
    "  range_amount = []\n",
    "  for i in range(num_bins):\n",
    "      lower_lim, higher_lim = str(int(bins[i])), str(int(bins[i+1]))\n",
    "      range_amount.append(\"$ \" + lower_lim + \" ~ \" + higher_lim )\n",
    "  df_hist = pd.DataFrame(index = range_amount)\n",
    "  df_hist.index.name = 'Transaction Amount[$]'\n",
    "  df_hist['# Total'] = n\n",
    "  df_hist['# Fraud'] = n_fraud\n",
    "  df_hist['# Safe'] = df_hist['# Total'] - df_hist['# Fraud']\n",
    "  df_hist['% Fraud'] = (df_hist['# Fraud'] / df_hist['# Total'] * 100).round(2)\n",
    "  df_hist['% Safe'] = (df_hist['# Safe'] / df_hist['# Total'] * 100).round(2)\n",
    "  print(df_hist)\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FREQUENCY DISTRIBUTION OF TRANSACTION AMOUNTS\" , 10*\"_ \"))  \n",
    "  fig3 = plt.figure(figsize=(16,6))\n",
    "  # Generating stacked bar-chart\n",
    "  bars_fraud = plt.bar(range(num_bins), df_hist['# Safe'], width = 0.5, color = '#00e64d')\n",
    "  bars_safe = plt.bar(range(num_bins), df_hist['# Fraud'], width = 0.5, bottom = df_hist['# Safe'], color='#ff6666')\n",
    "  # Labeling\n",
    "  plt.title(\"Figure 3. Frequency Distribution of Transaction Amount\", fontsize = 20)\n",
    "  plt.xticks(range(num_bins), range_amount, fontsize = 14)\n",
    "  plt.yticks(fontsize = 14)\n",
    "  plt.legend((bars_fraud[0], bars_safe[0]), ('Safe Transaction', 'Fraud'), loc=4, fontsize = 16)\n",
    "  plt.xlabel('Ranges of Transaction Amount', fontsize=16)\n",
    "  plt.ylabel('Number of Occurence', fontsize=16)\n",
    "  # hiding top/right border\n",
    "  ax = plt.gca()\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.spines['top'].set_visible(False)\n",
    "  x = plt.gca().xaxis\n",
    "  # rotate the tick labels for the x axis\n",
    "  for item in x.get_ticklabels():\n",
    "      item.set_rotation(50)\n",
    "  plt.show()\n",
    "  plt.savefig('FREQUENCY_DISTRIBUTION_OF_TRANSACTION_AMOUNTS_visualize_data.png')\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FRAUD PERCENTAGE AT DIFFERENT RANGES OF TRANSACTION AMOUNT\" , 10*\"_ \"))  \n",
    "  fig4 = plt.figure(figsize=(16,6))\n",
    "  # Generating stacked bar-chart\n",
    "  bars_fraud = plt.bar(range(num_bins), df_hist['% Safe'], width = 0.5, color = '#00e64d')\n",
    "  bars_safe = plt.bar(range(num_bins), df_hist['% Fraud'], width = 0.5, bottom = df_hist['% Safe'], color='#ff6666')\n",
    "  # Labeling\n",
    "  plt.title(\"Figure 4. Fraud Percentage at Different Ranges of Transaction Amount\", fontsize = 20)\n",
    "  plt.xticks(range(num_bins), range_amount, fontsize = 14)\n",
    "  plt.yticks(fontsize = 14)\n",
    "  plt.legend((bars_fraud[0], bars_safe[0]), ('Safe Transaction', 'Fraud'), loc=4, fontsize = 16)\n",
    "  plt.xlabel('Ranges of Transaction Amount', fontsize=16)\n",
    "  plt.ylabel('Percentage', fontsize=16)\n",
    "  plt.ylim(0, 100)\n",
    "  x = plt.gca().xaxis\n",
    "  # rotate the tick labels for the x axis\n",
    "  for item in x.get_ticklabels():\n",
    "      item.set_rotation(85)\n",
    "  # hiding top/right border\n",
    "  ax = plt.gca()\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.spines['top'].set_visible(False)    \n",
    "  # bar-value display\n",
    "  for bar in bars_safe:\n",
    "      plt.gca().text(bar.get_x() + bar.get_width()/2, bar.get_height() - 5, str(int(bar.get_height())) + '%', \n",
    "                  ha='center', color='w', fontsize=13, rotation = 'vertical', weight = 'bold')\n",
    "  plt.gca().text(bars_fraud[0].get_x() + bars_fraud[0].get_width()/2, bars_fraud[0].get_height() - 5, str(int(bars_fraud[0].get_height())) + '%', \n",
    "                  ha='center', color='black', fontsize=13, rotation = 'vertical', weight = 'bold')\n",
    "  plt.show()\n",
    "  plt.savefig('FRAUD_PERCENTAGE_AT_DIFFERENT_RANGES_OF_TRANSACTION_AMOUNT_visualize_data.png')\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FRAUD VS NON-FRAUD HISTOGRAM\" , 10*\"_ \"))  \n",
    "  plt.figure(figsize=(30,10))\n",
    "  sns.boxplot(x=data['category'],y=data.amount)\n",
    "  plt.title(\"Boxplot for the Amount spend in category\")\n",
    "  plt.ylim(0,4000)\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  plt.savefig('FRAUD_VS_NONFRAUD_HISTOGRAM_visualize_data.png')\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FRAUD DISTRIBUTION PER AGE\" , 10*\"_ \"))  \n",
    "  age_comparison_df = df_fraud.groupby('age')['fraud'].agg(['count']).reset_index().rename(columns={'age':'Age','count' : '# of Fraud'}).sort_values(by='# of Fraud')\n",
    "  age_df = pd.DataFrame([ [\"'0'\", \"<=18\"], [\"'1'\", \"19-25\"], [\"'2'\", \"26-35\"], [\"'3'\", \"36-45\"], [\"'4'\", \"46-55\"], [\"'5'\", \"56-65\"], [\"'6'\", \">65\"], [\"'U'\", \"Unknown\"], ], columns=[\"Age\", \"Label\"])\n",
    "  age_comparison_df = pd.merge(age_comparison_df, age_df, on=\"Age\", how=\"outer\")\n",
    "  age_comparison_df['Age'] = age_comparison_df['Age'].map(lambda x: x.strip(\"'\"))\n",
    "  age_comparison_df = age_comparison_df.sort_index(by=[\"Age\"])\n",
    "  age_comparison_df = age_comparison_df[[\"Age\", \"Label\", '# of Fraud']]\n",
    "  print(age_comparison_df)\n",
    "    \n",
    "\n",
    "def normalizing_data(data):\n",
    "  # Generate Hash Maps to be able to convert from numerical to categorical later.\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"CATEGORICAL VALUES TO NUMERICAL - HASHMAP GENERATION\" , 10*\"_ \"))\n",
    "  tmp_df = data[:]\n",
    "  col_categorical = tmp_df.select_dtypes(include= ['object']).columns\n",
    "  print( \"Features Types: \\n\\n{}\\n\\n\".format(tmp_df.dtypes) )\n",
    "  print( \"Categorical Features: {}\\n\\n\".format(col_categorical) )\n",
    "  print( \"\\nHash maps previews:\\n\" )\n",
    "  labels_hash = dict()\n",
    "  for col_name in col_categorical:                         \n",
    "    tmp_df[col_name] = tmp_df[col_name].astype('category') \n",
    "    labels_hash[col_name] = pd.DataFrame(  zip( tmp_df[col_name].cat.codes, tmp_df[col_name] ) , columns=[\"Index\", \"Label\"] ).drop_duplicates(subset=['Index'])\n",
    "    print(\"{} {} {} \\n {}\".format(10*\"_\", col_name , 10*\"_\", labels_hash[col_name].head() ) )\n",
    "  # Converting categorical entries to integers\n",
    "  tmp_df[col_categorical] = tmp_df[col_categorical].apply(lambda x: x.cat.codes)\n",
    "  # seperatign data columns and target columns\n",
    "  col_names = tmp_df.columns.tolist()\n",
    "  col_names_features = col_names[0:len(col_names)-1]\n",
    "  col_name_label     = col_names[-1]\n",
    "  # Declaring 'data-dataframe'  and 'target-dataframe'\n",
    "  X = tmp_df[col_names_features]\n",
    "  y = tmp_df[col_name_label]\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"FEATURE IMPORTANCE\" , 10*\"_ \"))\n",
    "  dt = DecisionTreeClassifier()\n",
    "  print(\"\\n\\n{}\\n\\n\".format(dt))\n",
    "  dt.fit(X, y)\n",
    "  # sorted-feature importances from the preliminary decision tree \n",
    "  ds_fi = pd.Series(dt.feature_importances_ , index = col_names_features).sort_values(ascending= False)\n",
    "  # plotting feature imporance bar-graph\n",
    "  fig2 = plt.figure(figsize=(13,5))\n",
    "  # Generating stacked bar-chart\n",
    "  bars_ft = plt.bar(range(len(ds_fi)), ds_fi, width = .8, color = '#2d64bc')\n",
    "  # Labeling\n",
    "  ttl = plt.title(\"Figure 2. Feature Importances\", fontsize = 20).set_position([0.45, 1.1])\n",
    "  plt.xticks(range(len(ds_fi)), ds_fi.index, fontsize = 14)\n",
    "  # plot-dejunking\n",
    "  ax = plt.gca()\n",
    "  ax.yaxis.set_visible(False) # hide entire y axis (both ticks and labels)\n",
    "  ax.xaxis.set_ticks_position('none')  # hide only the ticks and keep the label\n",
    "  plt.xticks(rotation='vertical')\n",
    "  # hide the frame\n",
    "  for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "  # value displaying\n",
    "  rects = ax.patches  \n",
    "  labels = ds_fi.values.round(2)\n",
    "  for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height, label, ha='center', va='bottom', fontsize = 13)\n",
    "  \n",
    "  plt.show()\n",
    "  plt.savefig('FEATURE_IMPORTANCE_normalizing_data.png')\n",
    "  return labels_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "yNL3liBmXUyW"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "nd9Y_-NCEF6z"
   },
   "outputs": [],
   "source": [
    "def generating3DRNNInput(data):\n",
    "  tmp_df = data[:]\n",
    "  col_categorical        = tmp_df.select_dtypes(include= ['object']).columns\n",
    "  for col_name in col_categorical:                         #????\n",
    "    tmp_df[col_name] = tmp_df[col_name].astype('category') \n",
    "  tmp_df[col_categorical]  = tmp_df[col_categorical].apply(lambda x: x.cat.codes)\n",
    "  X = tmp_df.iloc[:, :-1]\n",
    "  y = tmp_df.fraud\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"VISUALIZATION BEFORE TRANSFORMATION\" , 10*\"_ \"))\n",
    "  print(\"Total Fraud vs Non-Fraud Transactions Count: \\n\\n{}\\n\".format(y.value_counts()))\n",
    "  print(\"Number of customers: \",  X[\"customer\"].nunique() )\n",
    "  print(\"Ratio of positive frauds vs total dataset: {:0.2f}%\".format( ( y[y==1].count() /len(X)) *100  ))\n",
    "  # The RNNs requiere various a 3D input of groups of less of 300 samples per group for better performance. One option is to divide the datasets per day. \n",
    "  # Even by dividing per day, each day has more than 7K data points per data group.\n",
    "  mean_samples_per_customer = X[\"customer\"].value_counts().mean()\n",
    "  max_samples_per_customer = X[\"customer\"].value_counts().max()\n",
    "  print(\"\\nTransactions per customer.\\n\\tMean: {:0.1f}\\n\\tMax:  {:0.0f} \\n\\tNumber of Batches Using Max Amount as Fixed Size: {:0.1f} ~ {}\\n\\tNumber of Batches Using Mean Amount as Fixed Size: {:0.1f} ~ {}\".format(\n",
    "      mean_samples_per_customer, \n",
    "      max_samples_per_customer, \n",
    "      len(X) / max_samples_per_customer ,\n",
    "      math.ceil(len(X) / max_samples_per_customer ),\n",
    "      len(X) / mean_samples_per_customer ,\n",
    "      math.ceil(len(X) / mean_samples_per_customer )\n",
    "  ))\n",
    "  mean_samples_per_day = X[\"step\" ].value_counts().mean()\n",
    "  max_samples_per_day = X[\"step\"].value_counts().max()\n",
    "  print(\"\\n\\nSamples per Step (day): \\n\\tMean: {:0.0f} \\n\\tMax: {} \\n\\tNumber of Batches Using Max Amount as Fixed Size: {:0.1f} ~ {}\\n\\tNumber of Batches Using Mean Amount as Fixed Size: {:0.1f} ~ {}\".format(\n",
    "    mean_samples_per_day, max_samples_per_day, \n",
    "    len(X) / max_samples_per_day ,\n",
    "    math.ceil(len(X) / max_samples_per_day ),\n",
    "    len(X) / mean_samples_per_day ,\n",
    "    math.ceil(len(X) / mean_samples_per_day )\n",
    "  ))\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"GROUPING TRANSACTIONS BY CUSTOMER ID\" , 10*\"_ \"))\n",
    "  customer_batches = dict()\n",
    "  count = 0\n",
    "  for x in tmp_df.groupby([\"customer\"]):\n",
    "    customer_batches[x[0]] = x[1] \n",
    "  min_trans_per_cust = np.min( [ customer_batches[i].shape[0] for i in customer_batches]  ) \n",
    "  mean_trans_per_cust = np.mean( [ customer_batches[i].shape[0] for i in customer_batches]  ) \n",
    "  max_trans_per_cust = np.max( [ customer_batches[i].shape[0] for i in customer_batches]  ) \n",
    "  n_features =  tmp_df.shape[1]\n",
    "  print(\"LEN: \", len(customer_batches))\n",
    "  print(\"# Feaures: \", n_features )\n",
    "  print(\"MIN TRANS PER CUST: \", min_trans_per_cust )\n",
    "  print(\"MEAN TRANS PER CUST: \", mean_trans_per_cust, \" ~ \", math.ceil(mean_trans_per_cust) )\n",
    "  print(\"MAX TRANS PER CUST: \", max_trans_per_cust)\n",
    "  print(\"EXAMPLE: \\n\\n\", customer_batches[0] )\n",
    "\n",
    "  # See how many groups of customer transactions are above the average size of 145\n",
    "  print(\"\\n\\nSCATTER PLOT SHOWING SIZES OF BATCHES GROUPED BY CUSTOMER ID: \\n\\n\" )\n",
    "  fig=plt.figure()\n",
    "  ax=fig.add_axes([0,0,1,1])\n",
    "  ax.scatter(range(4112), [ customer_batches[i].shape[0] for i in customer_batches] , color='r')\n",
    "  ax.plot([0, 4112], [145, 145], \"--\")\n",
    "  ax.set_xlabel('Batch Index')\n",
    "  ax.set_ylabel('Batch Size')\n",
    "  ax.set_title('Customer Batches Sizes')\n",
    "  plt.show()\n",
    "  plt.savefig('SCATTER_PLOT_SHOWING_SIZES_OF_BATCHES_GROUPED_BY_CUSTOMER_ID_generating3DRNNInput.png')\n",
    "\n",
    "  print(\"\\n\\nBOX PLOT SHOWING MEAN SIZE OF BATCHES GROUPED BY CUSTOMER ID: \\n\\n\" )\n",
    "  fig1, ax1 = plt.subplots()\n",
    "  ax1.set_title('Batch Sizes')\n",
    "  ax1.boxplot([ customer_batches[i].shape[0] for i in customer_batches],  vert=False)\n",
    "  plt.show()\n",
    "  plt.savefig('BOX_PLOT_SHOWING_SIZES_OF_BATCHES_GROUPED_BY_CUSTOMER_ID_generating3DRNNInput.png')\n",
    "\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"GENERATING 3D INPUT WITH BATCHES OF SIZE \"+str(max_trans_per_cust) , 10*\"_ \"))\n",
    "  # Full 3D Array as an input for the LSTM\n",
    "  # empty_padding_value = -1\n",
    "  empty_padding_value = 0\n",
    "  np_customer_batches = list()\n",
    "  total_empty_rows_added = 0\n",
    "  for k in customer_batches:\n",
    "    empty_rows_to_add =  max_trans_per_cust - customer_batches[k].shape[0]\n",
    "    z = np.full( ( empty_rows_to_add , n_features ), empty_padding_value )\n",
    "    np_customer_batches.append( np.r_[ customer_batches[k].values , z] )\n",
    "    total_empty_rows_added += empty_rows_to_add\n",
    "  np_customer_batches_3d = np.array(np_customer_batches)\n",
    "  mean_frauds_per_batch       = np.mean( [ len(b[-1][b[-1] == 1 ]) for b in np_customer_batches_3d ]  )  \n",
    "  percentage_frauds_per_batch = np.mean( [ len(b[-1][b[-1] == 1 ]) / max_trans_per_cust for b in np_customer_batches_3d ]  )  * 100\n",
    "  print(\n",
    "      \"\"\"\n",
    "      The batches are separated by customer id. To be able to use the batches as input for the RNN, \n",
    "      it needs to have a static size. That is why the batch size is defined by the max number of \n",
    "      transactions done by the customers ({}). If one of the customers have done less transactions,\n",
    "      the rest of the empty space is filled with {} values. The final array size is {}.\\n\\n\"\"\".format(max_trans_per_cust, empty_padding_value, np_customer_batches_3d.shape ),\n",
    "      \"\\nTotal Empty Rows Added: \", total_empty_rows_added, \n",
    "      \"\\nPercentage of Empty Rows Added Compared to Total # of Data Points: %\", np.round(total_empty_rows_added / (max_trans_per_cust * len(np_customer_batches) ) * 100 , 2), \n",
    "      \"\\nNew Shape: \", np_customer_batches_3d.shape,\n",
    "      \"\\nMean of frauds per batch: \", np.round(mean_frauds_per_batch, 5),\n",
    "      \"\\nPercentage of frauds per batch: \", np.round(percentage_frauds_per_batch, 5),\n",
    "  )\n",
    "\n",
    "  np_customer_batches = list()\n",
    "  np_left_over_transactions = np.empty(shape=[0, n_features])\n",
    "  total_empty_rows_added = 0\n",
    "  mean_trans_per_cust = math.ceil(mean_trans_per_cust)\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"GENERATING 3D INPUT WITH BATCHES OF SIZE \"+str(mean_trans_per_cust) , 10*\"_ \"))\n",
    "  for k in customer_batches:\n",
    "    if( mean_trans_per_cust > customer_batches[k].shape[0] ):\n",
    "      empty_rows_to_add =  mean_trans_per_cust - customer_batches[k].shape[0]\n",
    "      z = np.full( ( empty_rows_to_add , n_features ) , empty_padding_value )\n",
    "      np_customer_batches.append( np.r_[ customer_batches[k].values , z] )\n",
    "      total_empty_rows_added += empty_rows_to_add\n",
    "    else:\n",
    "      np_customer_batches.append( np.array(customer_batches[k][0:mean_trans_per_cust].values) ) \n",
    "      np_left_over_transactions = np.r_[ np_left_over_transactions , customer_batches[k][mean_trans_per_cust:].values ]  #axis 0 to append vertically.\n",
    "  left_over_n_batches = math.ceil( len(np_left_over_transactions) / mean_trans_per_cust )\n",
    "  left_over_z = np.full( ( (left_over_n_batches * mean_trans_per_cust ) - len(np_left_over_transactions)  , n_features ),  empty_padding_value )\n",
    "  np_left_over_transactions    = np.r_[ np_left_over_transactions , left_over_z ] \n",
    "  np_left_over_transactions_3d = np.reshape(np_left_over_transactions, (left_over_n_batches, mean_trans_per_cust, n_features )  )\n",
    "  np_shifted_customer_batches_3d       = np.r_[ np.array(np_customer_batches) , np_left_over_transactions_3d ] \n",
    "  total_empty_rows = total_empty_rows_added + len(left_over_z)\n",
    "  mean_frauds_per_batch       = np.mean( [ len(b[-1][b[-1] == 1 ]) for b in np_shifted_customer_batches_3d ]  )  \n",
    "  percentage_frauds_per_batch = np.mean( [ len(b[-1][b[-1] == 1 ]) / mean_trans_per_cust for b in np_shifted_customer_batches_3d ]  )  * 100\n",
    "  print(\n",
    "      \"\"\"\n",
    "      The batches are separated by customer id. To be able to use the batches as input for the RNN, \n",
    "      it needs to have a static size. That is why the batch size is defined by the average number of \n",
    "      transactions done by the customers ({}). If one of the customers have done less transactions,\n",
    "      the rest of the empty space is filled with {} values. The final array size is {}. The difference\n",
    "      between this new more compacted version than previous which uses the max amount of transactions\n",
    "      per customers is that if a customer has more than the average number of transactions, these \n",
    "      transactions are saved in a separate array called left_overs. The left overs are then shaped as\n",
    "      a 3D array and appended to the main array. The problem with this array which is more efficient in \n",
    "      space and has less empty rows is that the mayority of batches are arranged by customer ID but the\n",
    "      last batches are in disorder, having transactions from many customers.\\n\\n\"\"\".format(mean_trans_per_cust, empty_padding_value, np_shifted_customer_batches_3d.shape ),\n",
    "      \"\\nTotal Empty Rows Added: \", total_empty_rows_added, \n",
    "      \"\\n% Empty Rows Added: %\", (total_empty_rows_added / (mean_trans_per_cust * len(np_customer_batches) ) * 100 ), \n",
    "      \"\\nNew Shape: \", np.array(np_customer_batches).shape,\n",
    "      \"\\nLeft overs: \", np_left_over_transactions.shape,\n",
    "      \"\\nLeft overs %: \", np.round( len(np_left_over_transactions)  / (mean_trans_per_cust * len(np_customer_batches) ) * 100, 1 ),\n",
    "      \"\\nLeft overs new Shape: \", np_left_over_transactions_3d.shape,\n",
    "      \"\\nLeft overs empty rows: \", len(left_over_z),\n",
    "      \"\\nLeft overs empty rows percentage (%) over total dataset: \", np.round( len(left_over_z)  / (mean_trans_per_cust * len(np_customer_batches) ) * 100, 4 ),\n",
    "      \"\\nTotal Empty # Rows and %: \", total_empty_rows, \" - \", ( total_empty_rows / (mean_trans_per_cust * len(np_customer_batches) ) ) * 100,\n",
    "      \"\\nMean of frauds per batch: \", mean_frauds_per_batch,\n",
    "      \"\\nPercentage of frauds per batch: \", percentage_frauds_per_batch,\n",
    "      \"\\nFinal 3D Array Shape (Customer Batches + Left overs): \", np_shifted_customer_batches_3d.shape,\n",
    "  )\n",
    "  return np_customer_batches_3d, np_shifted_customer_batches_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "bv63cayUT1QY"
   },
   "source": [
    "# Generating New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "TdER07w66D0N"
   },
   "source": [
    "**Generating new Features**\n",
    "\n",
    "1. Average amount ($) per customer & transaction type\n",
    "2. Average time since the last transaction for the same transaction type\n",
    "3. Average time per transaction for that particular transaction type\n",
    "4. Is this a new transaction for that particular type\n",
    "5. Is the transaction made to a new merchant\n",
    "\n",
    "The following functions will iterate through the data but only be able to filter the past data points before the current point to simulate that past historic data is available until such moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "I2kkm1x2T4hG"
   },
   "outputs": [],
   "source": [
    "def generateNewFeatures(data):\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"GENERATING NEW FEATURES\" , 10*\"_ \"))\n",
    "  print(\n",
    "    \"\"\"\n",
    "    The following features will created using the original Data. \n",
    "    The data generated is calculated inside each batch or group\n",
    "    or transactions grouped by custmer ID. Each calculation takes\n",
    "    all the data points before the current transaction in which the\n",
    "    loop index is currently located.\n",
    "\n",
    "    \\t Current day number of transactions  - \"curr_day_tr_n\"\n",
    "    \\t Average transaction amount per day  - \"ave_tr_p_day_amount\"\n",
    "    \\t Total average transaction amount \n",
    "    \\t From the beggining to current time  - \"tot_ave_tr_amount\"\n",
    "    \\t Is the merchant new?                - \"is_mer_new\"\n",
    "    \\t What is the common transaction type - \"com_tr_type\"\n",
    "    \\t What is the common merchant ID      - \"com_mer\"\n",
    "    \\n\\n\"\"\".format(),\n",
    "  )\n",
    "  # Customer ID column will be disposed. The ID is the same as the batch position in the array\n",
    "  columns=[\n",
    "    \"day\", \"age\", \"gender\", \"merchant\", \"category\", \"amount\", \n",
    "    \"curr_day_tr_n\",\"ave_tr_p_day_amount\", \"tot_ave_tr_amount\", \"is_mer_new\",\"com_tr_type\", \"com_mer\",\n",
    "    \"fraud\"\n",
    "  ]       #Original Cols : step ,customer , age, gender , merchant, category, amount , fraud \n",
    "  new_data = list()\n",
    "  start_time = time.time()\n",
    "  bar = progressbar.ProgressBar(max_value=len(data)) \n",
    "  print(30*\"_ \", \"\\n\\n\")\n",
    "  for b_i, b in enumerate( data ):\n",
    "    new_batch = pd.DataFrame(columns=columns)\n",
    "    for t_i, trans in enumerate( b ) :\n",
    "      step_col, merchant_col, cat_col, amount_col, is_fraud_col = 0, 4, 5, 6, 7\n",
    "      current_merchant = trans[merchant_col] # Merchant column\n",
    "      current_day      = trans[step_col]     # Day(step) column\n",
    "      current_cat      = trans[cat_col]     # Trans type column\n",
    "      is_new_merchant           = 0 if len( b[ :t_i, merchant_col ][ b[ :t_i, merchant_col ] == trans[merchant_col] ] ) > 0 else 1\n",
    "      ave_trans_amount          = np.around(np.mean( b[:t_i, amount_col ] ), 2 )\n",
    "      most_common_trans_type    =  np.bincount( b[:t_i,  cat_col ][b[:t_i, cat_col ] > 0].astype(int) ).argmax()  if  len(np.bincount( b[:t_i, cat_col ][b[:t_i, cat_col ] > 0].astype(int) )) > 0 else -1\n",
    "      most_common_merchant      =  np.bincount( b[:t_i,  merchant_col ][b[:t_i, merchant_col ] > 0].astype(int) ).argmax()  if  len(np.bincount( b[:t_i, merchant_col ][b[:t_i, merchant_col ] > 0].astype(int) )) > 0 else -1\n",
    "      ave_n_trans_per_day       =  np.round(pd.DataFrame(b[:t_i, [step_col, amount_col]][ b[:t_i, step_col ] != -1 ], index=None, columns=None).groupby(by=0).mean().reset_index().values[:, 1].mean() , 2)\n",
    "      n_trans_this_day          =  len( b[ :t_i + 1, step_col ][ b[ :t_i + 1, step_col ] == trans[step_col] ] )\n",
    "      ave_amount_for_curr_trans_type =  np.around(np.mean( b[:t_i+1, amount_col ][ b[:t_i+1, cat_col ] == trans[cat_col] ] ), 2 )\n",
    "      tr_data = {\n",
    "        \"day\": current_day, \"age\": trans[2], \"gender\": trans[3], \"merchant\": current_merchant, \"category\": current_cat, \"amount\" : trans[amount_col], \n",
    "        \"curr_day_tr_n\" : n_trans_this_day ,\"ave_tr_p_day_amount\": ave_n_trans_per_day, \"tot_ave_tr_amount\": ave_trans_amount, \"is_mer_new\": is_new_merchant, \"com_tr_type\" : most_common_trans_type, \"com_mer\": most_common_merchant,\n",
    "        \"fraud\" : trans[is_fraud_col]\n",
    "      }\n",
    "      new_batch = new_batch.append( tr_data , ignore_index=True)\n",
    "    # print(new_batch)\n",
    "    # break\n",
    "    new_data.append(new_batch.fillna(-1).values)\n",
    "    bar.update(b_i)\n",
    "  new_data = np.array(new_data)\n",
    "  delta_time = time.time() - start_time\n",
    "  print(\"--- {:0.2f} s seconds or {:0.2f} minutes ---\".format(delta_time, delta_time/60 ))\n",
    "  print(new_data.shape)\n",
    "  print(new_data[0])\n",
    "  return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "46dKTcGhT79W"
   },
   "source": [
    "# Oversampling with SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "3SzPmWPbc87w"
   },
   "source": [
    " (NOT NEEDED ANY MORE, THE CLASS WEIGHT METHOD WILL BE USED INSTEAD TO HANDLE IMBALANCE CLASSES)\n",
    "\n",
    "Using SMOTE(Synthetic Minority Oversampling Technique) for balancing the dataset. Resulted counts show that now we have exact number of class instances (1 and 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Mf3Qo-kTcmCw"
   },
   "outputs": [],
   "source": [
    "# sm = SMOTE(random_state=42)\n",
    "# X_res, y_res = sm.fit_resample(X, y)\n",
    "# y_res = pd.DataFrame(y_res)\n",
    "# X_res = pd.DataFrame(X_res)\n",
    "# print(\"\\n\\nSMOTE OVERSAMPLING. FRAUD AND NON-FRAUD NEW QUANTITIES\\n\\n\", y_res[0].value_counts(), \"\\n\\n X Shape: {} Y Shape: {}\".format( X_res.shape, y_res.shape ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "PwJKnbaSjY5j"
   },
   "outputs": [],
   "source": [
    "# X_oversampled = pd.DataFrame(\n",
    "#   X_res,\n",
    "# ).rename(columns = { 0:\"step\", 1:\"customer\",\t2:\"age\",\t3:\"gender\",\t4: \"merchant\",\t5: \"category\",\t6: \"amount\",\t7: \"typeTrans\"} )\n",
    "# X_oversampled = X_oversampled.astype({\"step\": int, \"customer\": int,\t\"age\": int,\t\"gender\": int,\t\"merchant\": int,\"category\": int,\t\"amount\": float, \"typeTrans\": int})\n",
    "# X_oversampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "n90mu0EDdcaA"
   },
   "source": [
    "# Data Split\n",
    "\n",
    "TRAIN 80% & TEST 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "w1Adi0s-BXG5"
   },
   "outputs": [],
   "source": [
    "def separateLabel(data):\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"SEPARATING X & y FOR TRAINING\" , 10*\"_ \"))\n",
    "  X = data[:, :, 0:-1]\n",
    "  y = data[:, :, -1]\n",
    "  #Replacing -1 values to 0\n",
    "  y[ data[:, :, -1] == -1] = 0\n",
    "  print(\"X Shape: {} Y Shape: {}\".format(X.shape, y.shape))\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "AtEj-NJudlIk"
   },
   "outputs": [],
   "source": [
    "def separatingTrainTest(X, y, test_size=0.2):\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"SEPARATING TEST & TRAIN\" , 10*\"_ \"))\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=test_size,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=None#y\n",
    "  )\n",
    "  print(\"\"\"\n",
    "  X-TRAIN Shape: {}\n",
    "  X-TEST Shape:  {}\n",
    "  Y-TRAIN Shape: {} #-Frauds: {} #-Non-Frauds: {}\n",
    "  Y-YEST Shape:  {} #-Frauds: {} #-Non-Frauds: {}\n",
    "  Total-#-Frauds: {} Total-#-Non-Frauds: {}\n",
    "  \\n\"\"\".format(\n",
    "      X_train.shape, X_test.shape, \n",
    "      y_train.shape, np.count_nonzero( y_train == 1 ), np.count_nonzero( y_train == 0 ),\n",
    "      y_test.shape,  np.count_nonzero( y_test  == 1 ), np.count_nonzero( y_test  == 0 ),\n",
    "      np.count_nonzero( y  == 1 ), np.count_nonzero( y  == 0 )\n",
    "  ))\n",
    "\n",
    "  return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "_kQ-jmAcwcJP"
   },
   "source": [
    "**10 K-Fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "7BLeXxhRdzMz"
   },
   "outputs": [],
   "source": [
    "# kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# kf.get_n_splits(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "6rlNdfHZs_m9"
   },
   "outputs": [],
   "source": [
    "# for index, (train_index, test_index) in enumerate(kf.split(X_res.values, y_res.values)):\n",
    "#   print(\"FOLD: \", index, \" => TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#   X_train, X_test, y_train, y_test = X_res.values[train_index], X_res.values[test_index], y_res.values[train_index], y_res.values[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "65Ow3HHl98wp"
   },
   "source": [
    "# Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "VZOi6ZBW92eU"
   },
   "outputs": [],
   "source": [
    "def normalize3DInput(data):\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"SEPARATING TEST & TRAIN\" , 10*\"_ \"))\n",
    "  n_batches        = data.shape[0]\n",
    "  batch_size       = data.shape[1]\n",
    "  n_features       = data.shape[2]\n",
    "  tmp_data         = data.reshape( (n_batches * batch_size, n_features ) )\n",
    "  print(\"Converting 3D to 2D for easy processing. Batch Sample: \\n\\n {} \\n\\n Original Array Shape: {}. Temporary array with shape: {}\\n\".format( data[0], data.shape, tmp_data.shape )) #(4112, 265, 12)\n",
    "\n",
    "  min_max_scaler = MinMaxScaler()\n",
    "  data_norm      = min_max_scaler.fit_transform(tmp_data)                      # ROBUST SCALER ANOTHER OPTION\n",
    "  data_norm      = data_norm.reshape( (n_batches, batch_size, n_features) )\n",
    "  print(\"Data Normalized and reshaped to a 3D array. Current Shape: {}. \\n\\nBatch Sample: \\n\\n {}\\n\\n\".format( data_norm.shape, data_norm[0] )) #(4112, 265, 12)\n",
    "\n",
    "  return data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "Bh5HaA-Y03g1"
   },
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "3c4l9yEt03Ff"
   },
   "outputs": [],
   "source": [
    "class MLModel():\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  def visualize_data(self, data):\n",
    "    pass\n",
    "  def preprocess(self, data):\n",
    "    X, y = None, None\n",
    "    return X, y\n",
    "  def create_model(self):\n",
    "    pass\n",
    "  def train(self, X, y):\n",
    "    pass\n",
    "  def evaluate( self, X_test, y_test ):\n",
    "    _, test_acc = self.model.evaluate( X_test, y_test , verbose=1)\n",
    "    print('Test Accuracy: {0.3f}'.format(test_acc))\n",
    "  def visualize(self):\n",
    "    pass\n",
    "    # GRAPH EXAMPLES https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "    pyplot.plot(self.history.history['loss'], label='train')\n",
    "    pyplot.plot(self.history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "nVQUWn_v7hfP"
   },
   "source": [
    "**LSTM & GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "hoaG1uOG7eeo"
   },
   "outputs": [],
   "source": [
    "class RNNModel(MLModel):\n",
    "  def __init__(\n",
    "      # GRID SEARCH & KERAS CLASSIFIER EXAMPLE https://www.kaggle.com/shujunge/gridsearchcv-with-keras\n",
    "      self, \n",
    "      input_shape,\n",
    "      output_dim,\n",
    "      param_grid={\n",
    "        \"rnn_hidden_layers\" : [1], \n",
    "        \"rnn_hidden_layers_neurons\" : [100], \n",
    "        \"hidden_layers\" : [0, 1, 2], \n",
    "        \"hidden_layers_neurons\" : [100, 200], \n",
    "        \"loss\" : ['binary_crossentropy'], \n",
    "        \"optimizer\" : ['adam'],\n",
    "        \"modelType\" : ['LSTM', 'GRU']\n",
    "      },\n",
    "      scoring=None, refit=True, verbose=2\n",
    "    ):\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"INITIALIZING GRID SEARCH RNN MODEL\" , 10*\"_ \"))\n",
    "    self.model = GridSearchCV(\n",
    "      estimator  = KerasClassifier( build_fn = self.create_model ),\n",
    "      param_grid = param_grid,\n",
    "      # scoring    = 'accuracy' , #['accuracy', 'precision'], \n",
    "      # refit      = 'precision',                  # For multiple metric evaluation, this needs to be a string denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.\n",
    "      n_jobs     = -1,                           # -1 means using all processors.\n",
    "      cv         = 10, \n",
    "      return_train_score = True,\n",
    "      scoring=scoring, \n",
    "      refit=refit, \n",
    "      verbose=verbose\n",
    "    )\n",
    "    self.input_shape = input_shape\n",
    "    self.output_dim  = output_dim\n",
    "    print(\"\"\"PARAMETERS:\\n________________________________\\ninput_shape: {}\\noutput_dim: {}\"\"\".format( input_shape, output_dim ))\n",
    "    for k in param_grid: print( \"{} : {}\".format(k, param_grid[k] ) )\n",
    "    print(\"\\n\\n\")\n",
    "  def create_model(self, hidden_layers, hidden_layers_neurons, loss, optimizer, rnn_hidden_layers, rnn_hidden_layers_neurons, modelType=\"LSTM\", dropout=True, dropout_rate=0.2  ):\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"CREATING ML MODEL\" , 10*\"_ \"))\n",
    "    print(\"\"\"\n",
    "    PARAMETERS:\n",
    "    ________________________________ \n",
    "      rnn_hidden_layers:         {} \n",
    "      rnn_hidden_layers_neurons: {} \n",
    "      hidden_layers:             {} \n",
    "      hidden_layers_neurons:     {}\n",
    "      loss:                      {}\n",
    "      optimizer:                 {}\n",
    "      modelType:                 {}\n",
    "      dropout:                   {}\n",
    "      dropout_rate:              {}\n",
    "      input_shape:               {}\n",
    "      output_dim:                {}\n",
    "      \\n\"\"\".format(rnn_hidden_layers, rnn_hidden_layers_neurons, hidden_layers, hidden_layers_neurons, loss, optimizer, modelType, dropout, dropout_rate, self.input_shape, self.output_dim ))\n",
    "    model = Sequential()\n",
    "    #INPUT DIM EXPLANATION https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "    for i in range(rnn_hidden_layers):\n",
    "      if(modelType == \"LSTM\"):\n",
    "        model.add(LSTM(units=rnn_hidden_layers_neurons , input_shape=self.input_shape,  ))\n",
    "      elif(modelType == \"GRU\"):\n",
    "        model.add(GRU( units=rnn_hidden_layers_neurons  ,input_shape=self.input_shape ))\n",
    "    for i in range(hidden_layers):\n",
    "      model.add(Dense(hidden_layers_neurons, activation='relu'))\n",
    "      if(dropout):\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(self.output_dim, activation='sigmoid')) # model.add(Dense(1, activation='softmax'))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    print(\"MODEL SUMMARY: \\n\\n\", model.summary())\n",
    "    self.modelType = modelType\n",
    "    self.model = model\n",
    "    return model\n",
    "  def train(self, X, y, X_test, y_test, output_file=\"RNN_best_model.h5\" ):\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"TRAINING RNN\" , 10*\"_ \"))\n",
    "    callbacks=[ \n",
    "       EarlyStopping(monitor='val_loss', mode='min', verbose=1) , \n",
    "       ModelCheckpoint( output_file , monitor='val_accuracy', mode='max', save_best_only=True, verbose=1),\n",
    "    ]\n",
    "    # CALLBACKS EXPLANATION  https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "    start=time.time()\n",
    "    # ClASS WEIGHTS COMPUTATION https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y.flatten()), y.flatten())\n",
    "    print(\"\\n\\nClass weights: {} \\nfor classes: {} \\n# Frauds: {}, \\n# of Non-Frauds: {}\\n\\n\".format( \n",
    "        class_weights, np.unique(y.flatten()), np.count_nonzero(y == 1), np.count_nonzero(y == 0) )  )\n",
    "    self.history = self.model.fit(\n",
    "      X, y, \n",
    "      validation_data=(X_test, y_test),\n",
    "      class_weight=class_weights\n",
    "    )\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \"RNN TRAINING RESULTS\" , 10*\"_ \"))\n",
    "    print(\"\"\"\n",
    "      BEST ESTIMATOR:          {} \n",
    "      BEST SCORE:              {}\n",
    "      BEST PARAMS:             {}\n",
    "      BEST INDEX IN CV SEARCH: {}\n",
    "      SCORER FUNCTIONS:        {}\n",
    "      \\n\n",
    "      HISTORY OBJ:             {}        \n",
    "    \\n\\n\"\"\".format( \n",
    "      self.history.best_estimator_,\n",
    "      self.history.best_score_ , \n",
    "      self.history.best_params_ ,\n",
    "      self.history.best_index_,\n",
    "      self.history.scorer_,\n",
    "      self.history\n",
    "    ))\n",
    "    print(\"cv_results_dict: \")\n",
    "    print(pd.DataFrame( self.history.cv_results_ ))\n",
    "    # for params, mean_score, scores in self.history.cv_results_:\n",
    "    # for params, mean_score, scores in self.history.grid_scores_:\n",
    "      # print(\"\\tMean: {}. Std: {}. Params: {}\".format(scores.mean(), scores.std(), params))\n",
    "    print(\"Total time: {:0.2f}  seconds or {:0.2f} minutes. Saving model to: {}\".format(  time.time()-start , (time.time()-start)/60, output_file ))\n",
    "    return self.history\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "fKdOjSKZk19v"
   },
   "source": [
    "**ROC AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "FF1ldTdD7-gL"
   },
   "outputs": [],
   "source": [
    "def plot_roc_auc(y_test, preds):\n",
    "    '''\n",
    "    Takes actual and predicted(probabilities) as input and plots the Receiver\n",
    "    Operating Characteristic (ROC) curve\n",
    "    '''\n",
    "    fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FVrt__ae7sa"
   },
   "source": [
    "# Generate Data & Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rro78HJsfACM"
   },
   "outputs": [],
   "source": [
    "def saveToCloud( X_train, X_test, y_train, y_test, labels_hash ):\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \" SAVE TO CLOUD \" , 10*\"_ \"))\n",
    "    \n",
    "    img_bucket_path   = project_path+\"/images\"\n",
    "    data_bucket_path  = project_path+\"/data\"\n",
    "    model_bucket_path = project_path+\"/models\"\n",
    "    \n",
    "    print(\"\\n\\nCOPYING IMAGES FILES ({})\\n\\n\".format(img_bucket_path))\n",
    "    !aws s3 cp \"/home/ec2-user/SageMaker/\" {img_bucket_path} --recursive --exclude=\"*\" --include=\"*.png\"\n",
    "    print(\"\\n\\nCOPYING DATA FILES ({})\\n\\n\".format(data_bucket_path))\n",
    "    !aws s3 cp \"/home/ec2-user/SageMaker/\" {data_bucket_path} --recursive --exclude=\"*\" --include=\"*.data\"\n",
    "    print(\"\\n\\nCOPYING MODEL FILES ({})\\n\\n\".format(model_bucket_path))\n",
    "    !aws s3 cp \"/home/ec2-user/SageMaker/\" {model_bucket_path} --recursive --exclude=\"*\" --include=\"*.h5\"\n",
    "    \n",
    "    print(\"\\n\\nImages Directory\\n\\n\")\n",
    "    try: \n",
    "        print( s3fs.S3FileSystem().ls(img_bucket_path) ); \n",
    "    except: \n",
    "        print(\"No Files In Folder.\")\n",
    "    print(\"\\n\\nData Directory\\n\\n\")\n",
    "    try: \n",
    "        print( s3fs.S3FileSystem().ls(data_bucket_path) ); \n",
    "    except: \n",
    "        print(\"No Files In Folder.\")\n",
    "    print(\"\\n\\nModel Directory\\n\\n\")\n",
    "    try: \n",
    "        print( s3fs.S3FileSystem().ls(model_bucket_path) );\n",
    "    except: \n",
    "        print(\"No Files In Folder.\")\n",
    "                   \n",
    "    #   !cp *.data \"/content/gdrive/My Drive/Verafin\"\n",
    "    #   !cp *.png \"/content/gdrive/My Drive/Verafin\"\n",
    "    #   !ls  \"/content/gdrive/My Drive/Verafin\"\n",
    "\n",
    "def generateData(save_to_cloud):\n",
    "  print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \" GENERATE DATA \" , 10*\"_ \"))\n",
    "\n",
    "  data                               = read_data(input_file_path=\"bs140513_032310.csv\")\n",
    "  visualize_data(data)\n",
    "  labels_hash                        = normalizing_data(data)\n",
    "  rnn_data, smaller_batches_rnn_data = generating3DRNNInput(data)\n",
    "  rnn_mod_data                       = generateNewFeatures(rnn_data)\n",
    "  X, y                               = separateLabel(rnn_mod_data)\n",
    "  X_norm                             = normalize3DInput(X)\n",
    "  \n",
    "  print(\"# Frauds {} vs # Non-Frauds {}. Total: {}\".format( \n",
    "    len(y[y==0]), len(y[y==1]), len(y.reshape(-1)) \n",
    "  )) \n",
    "  print(\"% Frauds {:0.2f}% vs % Non-Frauds {:0.2f}%\".format(\n",
    "    ( len(y[y==1]) / len(y.reshape(-1)) ) * 100, \n",
    "    ( len(y[y==0]) / len(y.reshape(-1)) ) * 100\n",
    "  ))\n",
    "  X_train, X_test, y_train, y_test = separatingTrainTest(X_norm, y)\n",
    "\n",
    "  pickle.dump( X_train      , open( \"X_train.data\"       , \"wb\" ) ) \n",
    "  pickle.dump( X_test       , open( \"X_test.data\"        , \"wb\" ) )\n",
    "  pickle.dump( y_train      , open( \"y_train.data\"       , \"wb\" ) )\n",
    "  pickle.dump( y_test       , open( \"y_test.data\"        , \"wb\" ) )\n",
    "  pickle.dump( labels_hash  , open( \"labels_hash.data\"   , \"wb\" ) )\n",
    "\n",
    "  if(save_to_cloud):\n",
    "    saveToCloud( X_train, X_test, y_train, y_test, labels_hash )\n",
    "\n",
    "  !ls *.png\n",
    "  !ls *.data\n",
    "  !ls *.h5\n",
    "\n",
    "  print(\"\"\"SHAPES & KEYS:\n",
    "  X_train          : {}\n",
    "  X_test           : {}\n",
    "  y_train          : {}\n",
    "  y_test           : {}\n",
    "  labels_hash Keys : {}\n",
    "  \"\"\".format(X_train.shape, X_test.shape, y_train.shape, y_test.shape, labels_hash.keys() ))\n",
    "\n",
    "  return X_train, X_test, y_train, y_test, labels_hash\n",
    "\n",
    "def readDataFromCloud():\n",
    "    print(\"\\n\\n{} {} {}\\n\\n\".format( 10*\"_ \" , \" READ DATA FROM CLOUD \" , 10*\"_ \"))\n",
    "    \n",
    "    data_bucket_path = project_path+\"/data\"\n",
    "    print(\"\\n\\nDownloading data from: \"+data_bucket_path+\"\\n\\n\")\n",
    "    !aws s3 cp {data_bucket_path}/X_train.data X_train.data\n",
    "    !aws s3 cp {data_bucket_path}/X_test.data  X_test.data\n",
    "    !aws s3 cp {data_bucket_path}/y_train.data y_train.data\n",
    "    !aws s3 cp {data_bucket_path}/y_test.data  y_test.data\n",
    "    !aws s3 cp {data_bucket_path}/labels_hash.data  labels_hash.data\n",
    "    \n",
    "    print(\"\\n\\nList the data files.\\n\\n\")\n",
    "    !pwd\n",
    "    !ls -la *.data\n",
    "\n",
    "#     root_path   = \"/content/gdrive/My Drive/Verafin/\"\n",
    "    root_path   = \"/home/ec2-user/SageMaker/\"\n",
    "    \n",
    "    X_train     = pickle.load( open( root_path+\"X_train.data\"       , \"rb\" ) ) \n",
    "    X_test      = pickle.load( open( root_path+\"X_test.data\"        , \"rb\" ) )\n",
    "    y_train     = pickle.load( open( root_path+\"y_train.data\"       , \"rb\" ) )\n",
    "    y_test      = pickle.load( open( root_path+\"y_test.data\"        , \"rb\" ) )\n",
    "    labels_hash = pickle.load( open( root_path+\"labels_hash.data\"   , \"rb\" ) )\n",
    "\n",
    "    print(\"\"\"\\n\\nSHAPES & KEYS:\n",
    "    X_train          : {}\n",
    "    X_test           : {}\n",
    "    y_train          : {}\n",
    "    y_test           : {}\n",
    "    labels_hash Keys : {}\n",
    "    \"\"\".format(X_train.shape, X_test.shape, y_train.shape, y_test.shape, labels_hash.keys() ))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, labels_hash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_n1-ZT8HDKnj"
   },
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "crzNZ0nmgn8Z",
    "outputId": "de14843c-ec13-4348-a0c4-dbba444a8eca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# !ls \"/content/gdrive/My Drive/Verafin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "VxnFaoS3oVfC",
    "outputId": "f24fbc72-43e5-46c1-fb37-9206c21c9677",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _   READ DATA FROM CLOUD  _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data from: s3://verafin-mitacs-ruben-chevez/customer_batches/data\n",
      "\n",
      "\n",
      "download: s3://verafin-mitacs-ruben-chevez/customer_batches/data/X_train.data to ./X_train.data\n",
      "download: s3://verafin-mitacs-ruben-chevez/customer_batches/data/X_test.data to ./X_test.data\n",
      "download: s3://verafin-mitacs-ruben-chevez/customer_batches/data/y_train.data to ./y_train.data\n",
      "download: s3://verafin-mitacs-ruben-chevez/customer_batches/data/y_test.data to ./y_test.data\n",
      "download: s3://verafin-mitacs-ruben-chevez/customer_batches/data/labels_hash.data to ./labels_hash.data\n",
      "\n",
      "\n",
      "List the data files.\n",
      "\n",
      "\n",
      "/home/ec2-user/SageMaker\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   164405 Feb 12 04:12 labels_hash.data\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 20937284 Feb 12 04:12 X_test.data\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 83672324 Feb 12 04:12 X_train.data\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  1744922 Feb 12 04:12 y_test.data\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  6972842 Feb 12 04:12 y_train.data\n",
      "\n",
      "\n",
      "SHAPES & KEYS:\n",
      "    X_train          : (3289, 265, 12)\n",
      "    X_test           : (823, 265, 12)\n",
      "    y_train          : (3289, 265)\n",
      "    y_test           : (823, 265)\n",
      "    labels_hash Keys : dict_keys(['customer', 'age', 'gender', 'merchant', 'category'])\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# %%capture output\n",
    "\n",
    "X_train, X_test, y_train, y_test, labels_hash = None, None, None, None, None\n",
    "\n",
    "if(generate_data):\n",
    "  X_train, X_test, y_train, y_test, labels_hash = generateData(save_to_cloud)\n",
    "else:\n",
    "  X_train, X_test, y_train, y_test, labels_hash = readDataFromCloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "dijlonzbEzGM",
    "outputId": "843301e8-13c3-460f-dc3d-36afe1d4d674",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _  INITIALIZING GRID SEARCH RNN MODEL _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "\n",
      "PARAMETERS:\n",
      "________________________________\n",
      "input_shape: (265, 12)\n",
      "output_dim: 265\n",
      "rnn_hidden_layers : [1]\n",
      "rnn_hidden_layers_neurons : [100]\n",
      "hidden_layers : [0]\n",
      "hidden_layers_neurons : [150]\n",
      "loss : ['mse']\n",
      "optimizer : ['adam']\n",
      "modelType : ['LSTM']\n",
      "epochs : [2]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_batches        = X_train.shape[0]\n",
    "batch_size       = X_train.shape[1]\n",
    "n_features       = X_train.shape[2]\n",
    "n_pred_per_batch = y_train.shape[1]\n",
    "rnn = RNNModel(\n",
    "  input_shape=( batch_size , n_features  ),\n",
    "  output_dim = n_pred_per_batch,\n",
    "  param_grid={\n",
    "    \"rnn_hidden_layers\" : [1], \n",
    "    \"rnn_hidden_layers_neurons\" : [100], \n",
    "    \"hidden_layers\" : [0], \n",
    "    \"hidden_layers_neurons\" : [150], \n",
    "    \"loss\" : [\"mse\"],       #['binary_crossentropy'], \n",
    "    \"optimizer\" : ['adam'],\n",
    "    \"modelType\" : ['LSTM'],\n",
    "    \"epochs\"    : [2]\n",
    "  },\n",
    "  scoring=None, refit=True, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xQYrKNEKFWR3",
    "outputId": "025d5de5-1bcb-4aa4-8f06-b5ae18bc3ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _  TRAINING RNN _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Class weights: [ 0.50332573 75.67155756] \n",
      "for classes: [0. 1.] \n",
      "# Frauds: 5759, \n",
      "# of Non-Frauds: 865826\n",
      "\n",
      "\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# %%capture output\n",
    "\n",
    "history = rnn.train( \n",
    "  X_train, y_train, X_test, y_test,\n",
    "  output_file= model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "id": "k0OtR6woQaRD",
    "outputId": "a77418cc-fab5-49ec-dda9-1249d8f196a1"
   },
   "outputs": [],
   "source": [
    "cv_results_df = pd.DataFrame(history.cv_results_).round(3)\n",
    "\n",
    "for i, cvr in enumerate(cv_results_df.iterrows()):\n",
    "  print(\"\"\"{} MODEL # {} {}\\n\n",
    "PARAM EPOCHS    :  {} HIDDEN LAYERS  :  {}   NEURONS PER HIDDEN LAYER:  {} \n",
    "LOSS FUNCTION   : \"{}\" MODEL TYPE     : \"{}\" OPTIMIZER        : \"{}\"\n",
    "STD FIT TIME    :  {} MEAN SCORE TIME:  {} STD SCORE TIME   :   {} \n",
    "MEAN TEST SCORE :  {} STD TEST SCORE :  {} RANK TEST SCORE  :   {} \n",
    "MEAN TRAIN SCORE:  {} STD TRAIN SCORE:  {} \n",
    "PARAMS: {}      \n",
    "\"\"\".format(\n",
    "    10*\" _\" , i+1, 10*\" _\", \n",
    "    cv_results_df.at[i, \"param_epochs\"]     , cv_results_df.at[i, \"param_hidden_layers\"]  , cv_results_df.at[i, \"param_hidden_layers_neurons\"], cv_results_df.at[i, \"param_rnn_hidden_layers\"] ,  cv_results_df.at[i, \"param_rnn_hidden_layers_neurons\"] , \n",
    "    cv_results_df.at[i, \"param_loss\"]       , cv_results_df.at[i, \"param_modelType\"]      , cv_results_df.at[i, \"param_optimizer\"]            ,\n",
    "    cv_results_df.at[i, \"std_fit_time\"]     , cv_results_df.at[i, \"mean_score_time\"]      , cv_results_df.at[i, \"std_score_time\"]             ,                                                                                                                      \n",
    "    cv_results_df.at[i, \"mean_test_score\"]  , cv_results_df.at[i, \"std_test_score\"]       , cv_results_df.at[i, \"rank_test_score\"]            ,\n",
    "    cv_results_df.at[i, \"mean_train_score\"] , cv_results_df.at[i, \"std_train_score\"]      ,\n",
    "    cv_results_df.at[i, \"params\"]                     \n",
    "  ))\n",
    "  train_score_p_split = [ cv_results_df.at[i,\"split{}_train_score\".format(j)] for j in range(10) ]\n",
    "  test_score_p_split  = [ cv_results_df.at[i,\"split{}_test_score\".format(j)] for j in range(10) ]\n",
    "  index_titles = [\"TRAIN\", \"TEST\"]\n",
    "\n",
    "  print(\"\"\"PERFORMANCE PER SPLIT \\n\\n{}\n",
    "  \\n\\n\"\"\".format( pd.DataFrame([ np.round(train_score_p_split, 3) , np.round(test_score_p_split, 3) ], \n",
    "    columns=[ \"SPLIT#{}\".format(se ) for se in range(10)],\n",
    "    index=index_titles\n",
    "  ) ))\n",
    "  \n",
    "  plt.plot(train_score_p_split)\n",
    "  plt.plot(test_score_p_split)\n",
    "  plt.title('model accuracy')\n",
    "  plt.suptitle( str(cv_results_df.at[i, \"params\"])  )\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('split')\n",
    "  plt.legend(['train', 'test'], loc='upper left')\n",
    "  plt.show()\n",
    "  img_name =\"Model#{}_{}\".format(i+1, str(cv_results_df.at[i, \"params\"]).upper())\n",
    "  for rt in ['{', '}', '\\'', ' ', ':', ',' ]:\n",
    "    img_name = img_name.replace( rt, '_')\n",
    "  print(\"\\n\\nSaving image with name: \",  img_name )\n",
    "  plt.savefig( img_name)\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "colab_type": "code",
    "id": "tPvyCue7f81r",
    "outputId": "04c9f3f1-b514-4754-da06-0a6edfde9d79"
   },
   "outputs": [],
   "source": [
    "selected_epochs = history.cv_results_[\"params\"][0]['epochs']\n",
    "print(\"\"\"\\n\\nBEST MODEL HISTORY PER EPOCH\n",
    "SELECTED EPOCHS   : {}\n",
    "PARAMS            : {} \\n\"\"\".format( \n",
    "  selected_epochs,\n",
    "  history.best_params_\n",
    "))\n",
    "\n",
    "index_names = [\"acc\", \"val_acc\", \"loss\", \"val_loss\"]\n",
    "index_titles = [\"TRAIN ACC\", \"TEST ACC\", \"TRAIN LOSS\", \"TEST LOSS\"]\n",
    "print( pd.DataFrame([ np.round( history.best_estimator_.model.history.history[iname] , 3) for iname in index_names], \n",
    "  columns=[ \"EPOCH#{}\".format(se+1) for se in range(selected_epochs)],\n",
    "  index=index_titles\n",
    "), \"\\n\")\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.best_estimator_.model.history.history['acc'])\n",
    "plt.plot(history.best_estimator_.model.history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig( 'BEST MODEL ACC' )\n",
    "print(\"\\n\\n\")\n",
    "# summarize history for loss\n",
    "plt.plot(history.best_estimator_.model.history.history['loss'])\n",
    "plt.plot(history.best_estimator_.model.history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig( 'BEST MODEL LOSS' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "7qTT8Q5oZ20-",
    "outputId": "c2ed1848-b1b0-4285-f742-bb9d91d8a075",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_hash.data\r\n"
     ]
    }
   ],
   "source": [
    "!ls *.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " saveToCloud( X_train, X_test, y_train, y_test, labels_hash )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kO_S_YizvNSd",
    "oKy1n6zEvGRP",
    "ttDcSRs_xrac",
    "hZcXZIqM-3uk",
    "yNL3liBmXUyW",
    "bv63cayUT1QY",
    "46dKTcGhT79W",
    "n90mu0EDdcaA",
    "65Ow3HHl98wp"
   ],
   "name": "RNN Fraud Detection tests.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Verafin MITACS 2020",
   "language": "python",
   "name": "verafin-mitacs-2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
